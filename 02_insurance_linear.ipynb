{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "02-insurance-linear.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushxx7/PyTorchCourse/blob/master/02_insurance_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfw78XlJV5q6"
      },
      "source": [
        "# Jovian Commit Essentials\n",
        "# Please retain and execute this cell without modifying the contents for `jovian.commit` to work\n",
        "!pip install jovian --upgrade -q\n",
        "import jovian\n",
        "jovian.utils.colab.set_colab_file_id('13vrlKp7WamOb-YOus_sPGRWX-tINRxe9')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "sB3eVbj8V5rA"
      },
      "source": [
        "# Insurance cost prediction using linear regression\n",
        "\n",
        "Make a submisson here: https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans/assignment/assignment-2-train-your-first-model\n",
        "\n",
        "In this assignment we're going to use information like a person's age, sex, BMI, no. of children and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance).\n",
        "\n",
        "\n",
        "We will create a model with the following steps:\n",
        "1. Download and explore the dataset\n",
        "2. Prepare the dataset for training\n",
        "3. Create a linear regression model\n",
        "4. Train the model to fit the data\n",
        "5. Make predictions using the trained model\n",
        "\n",
        "\n",
        "This assignment builds upon the concepts from the first 2 lessons. It will help to review these Jupyter notebooks:\n",
        "- PyTorch basics: https://jovian.ai/aakashns/01-pytorch-basics\n",
        "- Linear Regression: https://jovian.ai/aakashns/02-linear-regression\n",
        "- Logistic Regression: https://jovian.ai/aakashns/03-logistic-regression\n",
        "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n",
        "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n",
        "\n",
        "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JypuOptnV5rB"
      },
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder\n",
        "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy matplotlib pandas torch torchvision torchaudio"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDnV_z9kV5rB"
      },
      "source": [
        "import torch\n",
        "import jovian\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZtvYokIV5rC"
      },
      "source": [
        "project_name='02-insurance-linear-regression' # will be used by jovian.commit"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ6GhkIqV5rC"
      },
      "source": [
        "## Step 1: Download and explore the data\n",
        "\n",
        "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd7K3Lz4V5rC",
        "outputId": "a1e0bad2-6211-4103-e321-5f15e9ce4544"
      },
      "source": [
        "DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n",
        "DATA_FILENAME = \"insurance.csv\"\n",
        "download_url(DATASET_URL, '.')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./insurance.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQqbotvJV5rD"
      },
      "source": [
        "To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4RJerQEyV5rD",
        "outputId": "3312fb19-5498-4dcc-fa84-47bb8a22fd65"
      },
      "source": [
        "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
        "dataframe_raw.head()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age     sex     bmi  children smoker     region      charges\n",
              "0   19  female  27.900         0    yes  southwest  16884.92400\n",
              "1   18    male  33.770         1     no  southeast   1725.55230\n",
              "2   28    male  33.000         3     no  southeast   4449.46200\n",
              "3   33    male  22.705         0     no  northwest  21984.47061\n",
              "4   32    male  28.880         0     no  northwest   3866.85520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oRecAquV5rE"
      },
      "source": [
        "We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JsNKhbJV5rE"
      },
      "source": [
        "your_name = \"ayush\" # at least 5 characters"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_nkj0EpV5rE"
      },
      "source": [
        "The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F3oS6hQV5rE"
      },
      "source": [
        "def customize_dataset(dataframe_raw, rand_str):\n",
        "    dataframe = dataframe_raw.copy(deep=True)\n",
        "    # drop some rows\n",
        "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
        "    # scale input\n",
        "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n",
        "    # scale target\n",
        "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n",
        "    # drop column\n",
        "    if ord(rand_str[3]) % 2 == 1:\n",
        "        dataframe = dataframe.drop(['region'], axis=1)\n",
        "    return dataframe"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "s1G_4TZ4V5rE",
        "outputId": "779a4a94-4917-42e8-be1b-80dc086de022"
      },
      "source": [
        "dataframe = customize_dataset(dataframe_raw, your_name)\n",
        "dataframe.head()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>55</td>\n",
              "      <td>female</td>\n",
              "      <td>39.65775</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>14354.299732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>63</td>\n",
              "      <td>female</td>\n",
              "      <td>44.58850</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>16248.923145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>54</td>\n",
              "      <td>male</td>\n",
              "      <td>47.91600</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>12227.145840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>60</td>\n",
              "      <td>male</td>\n",
              "      <td>29.42720</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>14652.617616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>48</td>\n",
              "      <td>male</td>\n",
              "      <td>38.04845</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>10487.950843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age     sex       bmi  children smoker       charges\n",
              "27    55  female  39.65775         2     no  14354.299732\n",
              "997   63  female  44.58850         0     no  16248.923145\n",
              "162   54    male  47.91600         1     no  12227.145840\n",
              "824   60    male  29.42720         0     no  14652.617616\n",
              "392   48    male  38.04845         1     no  10487.950843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_uTm6BmV5rF"
      },
      "source": [
        "Let us answer some basic questions about the dataset. \n",
        "\n",
        "\n",
        "**Q: How many rows does the dataset have?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfgQu5TnV5rF",
        "outputId": "ca4a4bcc-3748-4aca-e2c9-f4d0ef42b154"
      },
      "source": [
        "num_rows = dataframe.shape[0]\n",
        "print(num_rows)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B4uekAcV5rF"
      },
      "source": [
        "**Q: How many columns doe the dataset have**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Zb6cNnV5rF",
        "outputId": "540dd83e-fccc-415a-aaf1-d3d35c720dde"
      },
      "source": [
        "num_cols = dataframe.shape[1]\n",
        "print(num_cols)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7FVzOqOV5rF"
      },
      "source": [
        "**Q: What are the column titles of the input variables?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVJi9ZbHV5rG"
      },
      "source": [
        "input_cols = ['age', 'sex', 'bmi', 'children', 'smoker']"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s6e9PP4V5rG"
      },
      "source": [
        "**Q: Which of the input columns are non-numeric or categorial variables ?**\n",
        "\n",
        "Hint: `sex` is one of them. List the columns that are not numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk_RRXDKV5rG"
      },
      "source": [
        "categorical_cols = ['sex', 'smoker']"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toMbMHEdV5rG"
      },
      "source": [
        "**Q: What are the column titles of output/target variable(s)?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaesnm7VV5rG"
      },
      "source": [
        "output_cols = ['charges']"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeUNKCNUV5rG"
      },
      "source": [
        "**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n",
        "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "jbuq6fClV5rH",
        "outputId": "8d504c7a-1d9c-4f91-b3fd-34a02d234512"
      },
      "source": [
        "minimum = dataframe.charges.min()\r\n",
        "maximum = dataframe.charges.max()\r\n",
        "average = dataframe.charges.mean()\r\n",
        "print(f\"The minimum, maximum & average value of charges is {minimum}, {maximum} & {average} respectively.\")\r\n",
        "\r\n",
        "g = sns.displot(dataframe.charges)\r\n",
        "g.fig.suptitle('Histogram')\r\n",
        "plt.show()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum, maximum & average value of charges is 1312.592463, 74611.4007717 & 15577.608507718964 respectively.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET3zFOkdV5rH"
      },
      "source": [
        "Remember to commit your notebook to Jovian after every step, so that you don't lose your work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLUDhoHlV5rH"
      },
      "source": [
        "# !pip install jovian --upgrade -q"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rxGjkbV5rH"
      },
      "source": [
        "# import jovian"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "NruHwlTlV5rH",
        "outputId": "652dc8b2-a122-4d77-a85f-358da307bf83"
      },
      "source": [
        "# jovian.commit(project=project_name)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "[jovian] Capturing environment..\u001b[0m\n",
            "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
            "[jovian] Committed successfully! https://jovian.ai/ayushxx7/02-insurance-linear-regression\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://jovian.ai/ayushxx7/02-insurance-linear-regression'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2qDyZWV5rH"
      },
      "source": [
        "## Step 2: Prepare the dataset for training\n",
        "\n",
        "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujb9icbV5rI"
      },
      "source": [
        "def dataframe_to_arrays(dataframe):\n",
        "    # Make a copy of the original dataframe\n",
        "    dataframe1 = dataframe.copy(deep=True)\n",
        "    # Convert non-numeric categorical columns to numbers\n",
        "    for col in categorical_cols:\n",
        "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
        "    # Extract input & outupts as numpy arrays\n",
        "    inputs_array = dataframe1[input_cols].to_numpy()\n",
        "    targets_array = dataframe1[output_cols].to_numpy()\n",
        "    return inputs_array, targets_array"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTGd2J6nV5rI"
      },
      "source": [
        "Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqIFhNqvV5rI",
        "outputId": "864f4cc3-ea4c-40ed-f598-b12af6bd98dc"
      },
      "source": [
        "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
        "inputs_array, targets_array"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[55.     ,  0.     , 39.65775,  2.     ,  0.     ],\n",
              "        [63.     ,  0.     , 44.5885 ,  0.     ,  0.     ],\n",
              "        [54.     ,  1.     , 47.916  ,  1.     ,  0.     ],\n",
              "        ...,\n",
              "        [58.     ,  1.     , 38.7321 ,  1.     ,  0.     ],\n",
              "        [32.     ,  0.     , 53.5062 ,  0.     ,  0.     ],\n",
              "        [35.     ,  1.     , 21.6106 ,  1.     ,  0.     ]]),\n",
              " array([[14354.2997325],\n",
              "        [16248.923145 ],\n",
              "        [12227.14584  ],\n",
              "        ...,\n",
              "        [13977.552303 ],\n",
              "        [ 4673.188026 ],\n",
              "        [ 5986.305468 ]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZqA2VMnV5rJ"
      },
      "source": [
        "**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VESZsOnuV5rJ"
      },
      "source": [
        "# inputs = torch.from_numpy(inputs_array)\n",
        "# targets = torch.from_numpy(targets_array)\n",
        "# The PyTorch Linear model crashed on dtype=float64\n",
        "inputs = torch.tensor(inputs_array, dtype=torch.float32)\n",
        "targets = torch.tensor(targets_array, dtype=torch.float32)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QK4kLjnV5rJ",
        "outputId": "020c36d0-254c-4a6c-e3dd-5aeb56be380c"
      },
      "source": [
        "inputs.dtype, targets.dtype"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMZRcumnVUZq",
        "outputId": "64a62d78-c8a0-4506-f281-6f2ba1083529"
      },
      "source": [
        "print(inputs[0:5])\r\n",
        "print(targets[0:5])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[55.0000,  0.0000, 39.6577,  2.0000,  0.0000],\n",
            "        [63.0000,  0.0000, 44.5885,  0.0000,  0.0000],\n",
            "        [54.0000,  1.0000, 47.9160,  1.0000,  0.0000],\n",
            "        [60.0000,  1.0000, 29.4272,  0.0000,  0.0000],\n",
            "        [48.0000,  1.0000, 38.0485,  1.0000,  0.0000]])\n",
            "tensor([[14354.2998],\n",
            "        [16248.9229],\n",
            "        [12227.1455],\n",
            "        [14652.6172],\n",
            "        [10487.9512]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AOodConV5rJ"
      },
      "source": [
        "Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73LLHxcV5rJ"
      },
      "source": [
        "dataset = TensorDataset(inputs, targets)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vupLrF4fV5rJ"
      },
      "source": [
        "**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTF66_BgV5rJ"
      },
      "source": [
        "val_percent = 0.2 # between 0.1 and 0.2\n",
        "val_size = int(num_rows * val_percent)\n",
        "train_size = num_rows - val_size\n",
        "\n",
        "\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpas_ULEq0cf",
        "outputId": "9b90fbb4-0614-45a5-a131-b6c5323de44a"
      },
      "source": [
        "print(len(train_ds), len(val_ds))\r\n",
        "print(train_size, val_size)\r\n",
        "print(len(dataset))\r\n",
        "print(1017+254)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1017 254\n",
            "1017 254\n",
            "1271\n",
            "1271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqQrc6Y-TuzO",
        "outputId": "b7cd1cf1-9d7e-451f-aeca-3dd699c1103c"
      },
      "source": [
        "print(train_ds[0:5], val_ds[0:5])"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[50.0000,  0.0000, 34.0736,  3.0000,  0.0000],\n",
            "        [58.0000,  0.0000, 27.5517,  0.0000,  0.0000],\n",
            "        [52.0000,  0.0000, 30.6130,  2.0000,  1.0000],\n",
            "        [53.0000,  1.0000, 25.8940,  1.0000,  0.0000],\n",
            "        [30.0000,  0.0000, 24.1395,  3.0000,  0.0000]]), tensor([[12522.0918],\n",
            "        [13845.5254],\n",
            "        [28860.8809],\n",
            "        [11776.5332],\n",
            "        [ 6661.3135]])) (tensor([[22.0000,  1.0000, 32.4764,  0.0000,  0.0000],\n",
            "        [59.0000,  1.0000, 49.7794,  1.0000,  1.0000],\n",
            "        [31.0000,  1.0000, 46.4519,  2.0000,  0.0000],\n",
            "        [48.0000,  0.0000, 33.1056,  1.0000,  0.0000],\n",
            "        [43.0000,  0.0000, 30.3468,  0.0000,  0.0000]]), tensor([[ 1948.0496],\n",
            "        [57295.1914],\n",
            "        [ 5221.9502],\n",
            "        [11053.4375],\n",
            "        [ 8570.3066]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHhI7-6VV5rK"
      },
      "source": [
        "Finally, we can create data loaders for training & validation.\n",
        "\n",
        "**Q: Pick a batch size for the data loader.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUx5qLTPV5rK"
      },
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO-yZ8PrV5rK"
      },
      "source": [
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cno70TEV5rK"
      },
      "source": [
        "Let's look at a batch of data to verify everything is working fine so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_YJhrcglV5rK",
        "outputId": "90dbbd65-d0b9-4c55-9b63-10e74ed9a08b"
      },
      "source": [
        "for xb, yb in train_loader:\n",
        "    print(\"inputs:\", xb)\n",
        "    print(\"targets:\", yb)\n",
        "    break"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs: tensor([[25.0000,  0.0000, 50.0033,  0.0000,  0.0000],\n",
            "        [24.0000,  1.0000, 39.5670,  0.0000,  1.0000],\n",
            "        [20.0000,  1.0000, 39.1979,  1.0000,  0.0000],\n",
            "        [25.0000,  1.0000, 31.2664,  1.0000,  0.0000],\n",
            "        [44.0000,  0.0000, 33.4505,  0.0000,  0.0000],\n",
            "        [47.0000,  0.0000, 28.5560,  1.0000,  0.0000],\n",
            "        [19.0000,  0.0000, 38.8531,  0.0000,  0.0000],\n",
            "        [22.0000,  1.0000, 63.6218,  1.0000,  1.0000],\n",
            "        [18.0000,  1.0000, 34.2551,  1.0000,  0.0000],\n",
            "        [64.0000,  1.0000, 31.9561,  0.0000,  0.0000],\n",
            "        [20.0000,  1.0000, 37.1288,  0.0000,  1.0000],\n",
            "        [53.0000,  0.0000, 47.9160,  1.0000,  0.0000],\n",
            "        [51.0000,  0.0000, 24.9260,  0.0000,  0.0000],\n",
            "        [50.0000,  1.0000, 41.3820,  2.0000,  1.0000],\n",
            "        [19.0000,  1.0000, 25.0470,  0.0000,  0.0000],\n",
            "        [60.0000,  1.0000, 44.7155,  0.0000,  0.0000],\n",
            "        [36.0000,  0.0000, 26.7833,  3.0000,  0.0000],\n",
            "        [32.0000,  0.0000, 35.9794,  0.0000,  0.0000],\n",
            "        [45.0000,  0.0000, 43.3362,  0.0000,  0.0000],\n",
            "        [21.0000,  1.0000, 42.9913,  0.0000,  0.0000],\n",
            "        [54.0000,  0.0000, 57.3661,  0.0000,  1.0000],\n",
            "        [30.0000,  1.0000, 29.5240,  3.0000,  1.0000],\n",
            "        [18.0000,  0.0000, 38.6232,  0.0000,  0.0000],\n",
            "        [38.0000,  1.0000, 24.1395,  1.0000,  0.0000],\n",
            "        [38.0000,  1.0000, 41.9870,  2.0000,  0.0000],\n",
            "        [54.0000,  1.0000, 41.3941,  2.0000,  1.0000],\n",
            "        [32.0000,  0.0000, 36.0580,  2.0000,  0.0000],\n",
            "        [45.0000,  0.0000, 30.4618,  2.0000,  0.0000],\n",
            "        [47.0000,  0.0000, 29.1610,  1.0000,  0.0000],\n",
            "        [30.0000,  0.0000, 37.3890,  3.0000,  0.0000],\n",
            "        [48.0000,  0.0000, 33.7953,  4.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 36.3363,  1.0000,  0.0000],\n",
            "        [24.0000,  1.0000, 32.4159,  1.0000,  0.0000],\n",
            "        [35.0000,  1.0000, 34.9690,  3.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 25.9787,  0.0000,  0.0000],\n",
            "        [59.0000,  1.0000, 36.0943,  3.0000,  1.0000],\n",
            "        [43.0000,  0.0000, 43.1244,  1.0000,  0.0000],\n",
            "        [22.0000,  0.0000, 48.1641,  0.0000,  0.0000],\n",
            "        [58.0000,  0.0000, 47.2505,  0.0000,  0.0000],\n",
            "        [31.0000,  0.0000, 31.2180,  2.0000,  0.0000],\n",
            "        [50.0000,  0.0000, 55.7689,  1.0000,  0.0000],\n",
            "        [57.0000,  1.0000, 40.6923,  1.0000,  0.0000],\n",
            "        [55.0000,  0.0000, 39.6577,  2.0000,  0.0000],\n",
            "        [60.0000,  0.0000, 33.3355,  0.0000,  0.0000],\n",
            "        [22.0000,  1.0000, 34.2551,  1.0000,  0.0000],\n",
            "        [51.0000,  1.0000, 40.3293,  3.0000,  0.0000],\n",
            "        [54.0000,  0.0000, 27.8300,  3.0000,  0.0000],\n",
            "        [61.0000,  0.0000, 44.0258,  1.0000,  1.0000],\n",
            "        [53.0000,  1.0000, 35.6708,  0.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 27.8179,  0.0000,  0.0000],\n",
            "        [25.0000,  1.0000, 31.1454,  0.0000,  0.0000],\n",
            "        [19.0000,  1.0000, 26.3235,  0.0000,  0.0000],\n",
            "        [26.0000,  1.0000, 37.3587,  2.0000,  0.0000],\n",
            "        [50.0000,  1.0000, 30.6130,  0.0000,  0.0000],\n",
            "        [37.0000,  0.0000, 30.9216,  1.0000,  1.0000],\n",
            "        [64.0000,  0.0000, 48.0370,  0.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 26.0937,  0.0000,  1.0000],\n",
            "        [58.0000,  0.0000, 38.5083,  2.0000,  0.0000],\n",
            "        [18.0000,  0.0000, 48.7146,  0.0000,  0.0000],\n",
            "        [46.0000,  1.0000, 51.2435,  3.0000,  1.0000],\n",
            "        [51.0000,  1.0000, 39.0830,  1.0000,  0.0000],\n",
            "        [42.0000,  0.0000, 30.2318,  2.0000,  0.0000],\n",
            "        [35.0000,  0.0000, 37.5100,  1.0000,  0.0000],\n",
            "        [42.0000,  1.0000, 31.8412,  1.0000,  0.0000],\n",
            "        [63.0000,  1.0000, 38.0485,  0.0000,  0.0000],\n",
            "        [35.0000,  1.0000, 33.4081,  1.0000,  0.0000],\n",
            "        [44.0000,  0.0000, 31.2180,  1.0000,  0.0000],\n",
            "        [52.0000,  0.0000, 45.4053,  2.0000,  0.0000],\n",
            "        [31.0000,  1.0000, 43.9230,  2.0000,  1.0000],\n",
            "        [54.0000,  0.0000, 56.5070,  2.0000,  0.0000],\n",
            "        [29.0000,  0.0000, 26.4385,  0.0000,  1.0000],\n",
            "        [52.0000,  0.0000, 56.5675,  5.0000,  0.0000],\n",
            "        [34.0000,  1.0000, 43.3362,  0.0000,  0.0000],\n",
            "        [26.0000,  0.0000, 36.2032,  1.0000,  0.0000],\n",
            "        [46.0000,  1.0000, 47.7043,  1.0000,  0.0000],\n",
            "        [49.0000,  0.0000, 38.5990,  5.0000,  0.0000],\n",
            "        [45.0000,  0.0000, 37.3890,  2.0000,  0.0000],\n",
            "        [47.0000,  1.0000, 30.7461,  1.0000,  1.0000],\n",
            "        [47.0000,  1.0000, 23.2199,  1.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 41.6603,  0.0000,  0.0000],\n",
            "        [60.0000,  1.0000, 31.1454,  0.0000,  0.0000],\n",
            "        [25.0000,  1.0000, 33.3355,  0.0000,  0.0000],\n",
            "        [48.0000,  0.0000, 34.9448,  1.0000,  0.0000],\n",
            "        [45.0000,  0.0000, 42.7130,  0.0000,  0.0000],\n",
            "        [47.0000,  0.0000, 33.4505,  2.0000,  1.0000],\n",
            "        [30.0000,  0.0000, 33.7953,  0.0000,  0.0000],\n",
            "        [56.0000,  1.0000, 41.6603,  0.0000,  0.0000],\n",
            "        [46.0000,  1.0000, 24.0245,  0.0000,  0.0000],\n",
            "        [33.0000,  1.0000, 40.4624,  5.0000,  0.0000],\n",
            "        [58.0000,  0.0000, 50.7111,  0.0000,  0.0000],\n",
            "        [53.0000,  0.0000, 44.6006,  3.0000,  1.0000],\n",
            "        [33.0000,  1.0000, 42.6465,  0.0000,  0.0000],\n",
            "        [39.0000,  1.0000, 51.6125,  0.0000,  0.0000],\n",
            "        [23.0000,  0.0000, 34.0252,  0.0000,  0.0000],\n",
            "        [39.0000,  1.0000, 31.9561,  0.0000,  1.0000],\n",
            "        [47.0000,  0.0000, 32.1860,  2.0000,  0.0000],\n",
            "        [39.0000,  0.0000, 39.3250,  1.0000,  0.0000],\n",
            "        [18.0000,  1.0000, 34.4850,  0.0000,  0.0000],\n",
            "        [21.0000,  1.0000, 28.7375,  2.0000,  0.0000],\n",
            "        [37.0000,  0.0000, 20.9209,  2.0000,  0.0000]])\n",
            "targets: tensor([[20918.3145],\n",
            "        [40333.2227],\n",
            "        [ 2763.8081],\n",
            "        [ 3872.4573],\n",
            "        [ 8682.7979],\n",
            "        [ 9991.4150],\n",
            "        [ 2492.8909],\n",
            "        [52066.6367],\n",
            "        [13188.6279],\n",
            "        [16841.6328],\n",
            "        [39166.7070],\n",
            "        [12378.2617],\n",
            "        [10839.8125],\n",
            "        [50142.5000],\n",
            "        [ 1454.0947],\n",
            "        [14907.1660],\n",
            "        [ 8457.0127],\n",
            "        [ 5097.7412],\n",
            "        [ 9046.2734],\n",
            "        [ 1792.9895],\n",
            "        [74611.3984],\n",
            "        [21363.2832],\n",
            "        [ 2580.9976],\n",
            "        [ 6851.4058],\n",
            "        [ 7116.4141],\n",
            "        [51785.0781],\n",
            "        [ 6027.9966],\n",
            "        [10641.2295],\n",
            "        [30696.7988],\n",
            "        [ 6231.0117],\n",
            "        [12887.7549],\n",
            "        [ 2012.8138],\n",
            "        [14753.5674],\n",
            "        [ 6934.4097],\n",
            "        [ 1991.8727],\n",
            "        [35316.3750],\n",
            "        [ 8594.5000],\n",
            "        [ 3223.3745],\n",
            "        [13872.0010],\n",
            "        [ 5773.6050],\n",
            "        [11172.9912],\n",
            "        [13975.8057],\n",
            "        [14354.2998],\n",
            "        [15464.0010],\n",
            "        [ 3087.6802],\n",
            "        [12355.7754],\n",
            "        [14150.5391],\n",
            "        [56765.5508],\n",
            "        [11100.5439],\n",
            "        [ 1994.3447],\n",
            "        [ 2501.0547],\n",
            "        [ 1903.9204],\n",
            "        [ 4536.4458],\n",
            "        [ 9877.9199],\n",
            "        [23747.3301],\n",
            "        [16753.2656],\n",
            "        [16085.0107],\n",
            "        [15920.6211],\n",
            "        [ 1912.4509],\n",
            "        [53996.8164],\n",
            "        [11657.9502],\n",
            "        [ 9379.9619],\n",
            "        [ 6131.6948],\n",
            "        [ 8120.8647],\n",
            "        [16350.1133],\n",
            "        [ 5554.0518],\n",
            "        [ 8920.8174],\n",
            "        [39162.2070],\n",
            "        [45291.8711],\n",
            "        [13499.9521],\n",
            "        [18854.9062],\n",
            "        [14733.2656],\n",
            "        [ 5054.8809],\n",
            "        [ 3969.7830],\n",
            "        [ 9761.2031],\n",
            "        [13516.8975],\n",
            "        [ 9968.4307],\n",
            "        [25715.0527],\n",
            "        [10094.2227],\n",
            "        [ 1330.8396],\n",
            "        [14206.8174],\n",
            "        [ 2952.1084],\n",
            "        [10821.9092],\n",
            "        [ 8597.3262],\n",
            "        [28706.7676],\n",
            "        [ 4840.9014],\n",
            "        [12395.2441],\n",
            "        [ 8806.2461],\n",
            "        [ 7784.9326],\n",
            "        [28345.9844],\n",
            "        [54593.8867],\n",
            "        [14513.7090],\n",
            "        [ 6736.1738],\n",
            "        [ 3147.4331],\n",
            "        [23574.7070],\n",
            "        [11367.5342],\n",
            "        [ 7298.8086],\n",
            "        [ 2003.3055],\n",
            "        [ 3600.2017],\n",
            "        [ 8047.2368]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNx6Pz33V5rK"
      },
      "source": [
        "Let's save our work by committing to Jovian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFKMrAzdV5rL"
      },
      "source": [
        "# jovian.commit(project=project_name, environment=None)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SomqIMMV5rL"
      },
      "source": [
        "## Step 3: Create a Linear Regression Model\n",
        "\n",
        "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrxeilGXV5rL"
      },
      "source": [
        "input_size = len(input_cols)\n",
        "output_size = len(output_cols)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGdmCM26V5rL"
      },
      "source": [
        "**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n",
        "\n",
        "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YblInuzPI3Of",
        "outputId": "b8014291-4996-48c2-d3d0-15c4f67f3d41"
      },
      "source": [
        "x = inputs.shape\r\n",
        "y = targets.shape\r\n",
        "print(x, y)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1271, 5]) torch.Size([1271, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXF0RTANV5rL"
      },
      "source": [
        "class InsuranceModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n",
        "        self.loss_fn = F.l1_loss\n",
        "        # tried mse_loss, hinge_embedding_loss, poisson_nll_loss\n",
        "        # tried but failed due to error ctc_loss, cosine_embedding_loss\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.linear(xb)                          # fill this\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        inputs, targets = batch \n",
        "        # Generate predictions\n",
        "        out = self(inputs)    # current object is the model, so pass in self directly        \n",
        "        # Calcuate loss\n",
        "        loss = self.loss_fn(out, targets)                          # fill this\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        inputs, targets = batch\n",
        "        # Generate predictions\n",
        "        out = self(inputs) \n",
        "        # Calculate loss\n",
        "        loss = self.loss_fn(out, targets)                          # fill this\n",
        "        # implement accurary later on    \n",
        "        return {'val_loss': loss.detach()}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result, num_epochs):\n",
        "        # Print result every 20th epoch\n",
        "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
        "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9l2e5QOV5rL"
      },
      "source": [
        "Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZSNMGz8V5rM"
      },
      "source": [
        "model = InsuranceModel()"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoxYRXPuV5rM"
      },
      "source": [
        "Let's check out the weights and biases of the model using `model.parameters`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gUVtjIQV5rM",
        "outputId": "0f72291b-58e5-4d21-963a-3c190ff5b7f3"
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.3042,  0.0130, -0.0085, -0.3482,  0.2885]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.2646], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4OSvvf0V5rM"
      },
      "source": [
        "One final commit before we train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "PHP61i92V5rM",
        "outputId": "b5b9ca54-6de2-4176-fcb5-861af627c467"
      },
      "source": [
        "# jovian.commit(project=project_name, environment=None)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
            "[jovian] Committed successfully! https://jovian.ai/ayushxx7/02-insurance-linear-regression\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://jovian.ai/ayushxx7/02-insurance-linear-regression'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZgtF441V5rN"
      },
      "source": [
        "## Step 4: Train the model to fit the data\n",
        "\n",
        "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUA1aGMV5rN"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result, epochs)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPwz3io1V5rN"
      },
      "source": [
        "**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4dUCsmbV5rN",
        "outputId": "e0e19e7a-1d91-4997-c9ee-7e894e3b6717"
      },
      "source": [
        "result = evaluate(model, val_ds) # Use the the evaluate function\n",
        "print(result)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'val_loss': 15393.3603515625}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T4yPd0JV5rN"
      },
      "source": [
        "\n",
        "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8UAi4nEV5rN"
      },
      "source": [
        "**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n",
        "\n",
        "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IviDloNTV5rN",
        "outputId": "8fe7d490-4aba-4b78-a5b2-18e182f6938b"
      },
      "source": [
        "epochs = 1000\n",
        "lr = 1e-6\n",
        "history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 15797.0469\n",
            "Epoch [40], val_loss: 15796.3955\n",
            "Epoch [60], val_loss: 15795.7451\n",
            "Epoch [80], val_loss: 15795.0928\n",
            "Epoch [100], val_loss: 15794.4424\n",
            "Epoch [120], val_loss: 15793.7891\n",
            "Epoch [140], val_loss: 15793.1396\n",
            "Epoch [160], val_loss: 15792.4873\n",
            "Epoch [180], val_loss: 15791.8359\n",
            "Epoch [200], val_loss: 15791.1846\n",
            "Epoch [220], val_loss: 15790.5342\n",
            "Epoch [240], val_loss: 15789.8799\n",
            "Epoch [260], val_loss: 15789.2295\n",
            "Epoch [280], val_loss: 15788.5771\n",
            "Epoch [300], val_loss: 15787.9248\n",
            "Epoch [320], val_loss: 15787.2734\n",
            "Epoch [340], val_loss: 15786.6221\n",
            "Epoch [360], val_loss: 15785.9717\n",
            "Epoch [380], val_loss: 15785.3203\n",
            "Epoch [400], val_loss: 15784.6689\n",
            "Epoch [420], val_loss: 15784.0186\n",
            "Epoch [440], val_loss: 15783.3672\n",
            "Epoch [460], val_loss: 15782.7139\n",
            "Epoch [480], val_loss: 15782.0625\n",
            "Epoch [500], val_loss: 15781.4111\n",
            "Epoch [520], val_loss: 15780.7588\n",
            "Epoch [540], val_loss: 15780.1064\n",
            "Epoch [560], val_loss: 15779.4561\n",
            "Epoch [580], val_loss: 15778.8037\n",
            "Epoch [600], val_loss: 15778.1533\n",
            "Epoch [620], val_loss: 15777.5010\n",
            "Epoch [640], val_loss: 15776.8486\n",
            "Epoch [660], val_loss: 15776.1982\n",
            "Epoch [680], val_loss: 15775.5469\n",
            "Epoch [700], val_loss: 15774.8936\n",
            "Epoch [720], val_loss: 15774.2422\n",
            "Epoch [740], val_loss: 15773.5908\n",
            "Epoch [760], val_loss: 15772.9375\n",
            "Epoch [780], val_loss: 15772.2861\n",
            "Epoch [800], val_loss: 15771.6357\n",
            "Epoch [820], val_loss: 15770.9844\n",
            "Epoch [840], val_loss: 15770.3330\n",
            "Epoch [860], val_loss: 15769.6826\n",
            "Epoch [880], val_loss: 15769.0312\n",
            "Epoch [900], val_loss: 15768.3799\n",
            "Epoch [920], val_loss: 15767.7295\n",
            "Epoch [940], val_loss: 15767.0781\n",
            "Epoch [960], val_loss: 15766.4268\n",
            "Epoch [980], val_loss: 15765.7764\n",
            "Epoch [1000], val_loss: 15765.1250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9QneyHAV5rO",
        "outputId": "0a201e52-4580-412b-f8cc-d6097e611bc8"
      },
      "source": [
        "epochs = 500\n",
        "lr = 1e-5\n",
        "history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 15758.6094\n",
            "Epoch [40], val_loss: 15752.1045\n",
            "Epoch [60], val_loss: 15745.5947\n",
            "Epoch [80], val_loss: 15739.0703\n",
            "Epoch [100], val_loss: 15732.5547\n",
            "Epoch [120], val_loss: 15726.0469\n",
            "Epoch [140], val_loss: 15719.5361\n",
            "Epoch [160], val_loss: 15713.0264\n",
            "Epoch [180], val_loss: 15706.5127\n",
            "Epoch [200], val_loss: 15699.9971\n",
            "Epoch [220], val_loss: 15693.4766\n",
            "Epoch [240], val_loss: 15686.9658\n",
            "Epoch [260], val_loss: 15680.4541\n",
            "Epoch [280], val_loss: 15673.9404\n",
            "Epoch [300], val_loss: 15667.4287\n",
            "Epoch [320], val_loss: 15660.9189\n",
            "Epoch [340], val_loss: 15654.4033\n",
            "Epoch [360], val_loss: 15647.8799\n",
            "Epoch [380], val_loss: 15641.3721\n",
            "Epoch [400], val_loss: 15634.8564\n",
            "Epoch [420], val_loss: 15628.3359\n",
            "Epoch [440], val_loss: 15621.8154\n",
            "Epoch [460], val_loss: 15615.3018\n",
            "Epoch [480], val_loss: 15608.7734\n",
            "Epoch [500], val_loss: 15602.2617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN23-DAKV5rO"
      },
      "source": [
        "# 1e-2 gives nan\n",
        "# epochs = 500\n",
        "# lr = 1e-2\n",
        "# history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxXD_t03V5rO",
        "outputId": "78832318-2d3f-49b9-c883-c8f8ff668c34"
      },
      "source": [
        "epochs = 1000\n",
        "lr = 1e-4\n",
        "history4 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 15537.1416\n",
            "Epoch [40], val_loss: 15472.0361\n",
            "Epoch [60], val_loss: 15406.9111\n",
            "Epoch [80], val_loss: 15341.8799\n",
            "Epoch [100], val_loss: 15276.7334\n",
            "Epoch [120], val_loss: 15211.5205\n",
            "Epoch [140], val_loss: 15146.2812\n",
            "Epoch [160], val_loss: 15081.1318\n",
            "Epoch [180], val_loss: 15015.9990\n",
            "Epoch [200], val_loss: 14950.9062\n",
            "Epoch [220], val_loss: 14885.7256\n",
            "Epoch [240], val_loss: 14820.6318\n",
            "Epoch [260], val_loss: 14755.4893\n",
            "Epoch [280], val_loss: 14690.2422\n",
            "Epoch [300], val_loss: 14625.0576\n",
            "Epoch [320], val_loss: 14559.8945\n",
            "Epoch [340], val_loss: 14494.7686\n",
            "Epoch [360], val_loss: 14429.7295\n",
            "Epoch [380], val_loss: 14364.6748\n",
            "Epoch [400], val_loss: 14299.6064\n",
            "Epoch [420], val_loss: 14234.6982\n",
            "Epoch [440], val_loss: 14169.9502\n",
            "Epoch [460], val_loss: 14105.1924\n",
            "Epoch [480], val_loss: 14040.7549\n",
            "Epoch [500], val_loss: 13976.7158\n",
            "Epoch [520], val_loss: 13912.8213\n",
            "Epoch [540], val_loss: 13848.9844\n",
            "Epoch [560], val_loss: 13785.2461\n",
            "Epoch [580], val_loss: 13721.6797\n",
            "Epoch [600], val_loss: 13658.4150\n",
            "Epoch [620], val_loss: 13595.5039\n",
            "Epoch [640], val_loss: 13533.1924\n",
            "Epoch [660], val_loss: 13471.8174\n",
            "Epoch [680], val_loss: 13411.5479\n",
            "Epoch [700], val_loss: 13351.8643\n",
            "Epoch [720], val_loss: 13292.3232\n",
            "Epoch [740], val_loss: 13232.8271\n",
            "Epoch [760], val_loss: 13173.6045\n",
            "Epoch [780], val_loss: 13115.3359\n",
            "Epoch [800], val_loss: 13057.1729\n",
            "Epoch [820], val_loss: 12999.4414\n",
            "Epoch [840], val_loss: 12942.3047\n",
            "Epoch [860], val_loss: 12886.2266\n",
            "Epoch [880], val_loss: 12831.1201\n",
            "Epoch [900], val_loss: 12776.9834\n",
            "Epoch [920], val_loss: 12723.6650\n",
            "Epoch [940], val_loss: 12671.1602\n",
            "Epoch [960], val_loss: 12618.9873\n",
            "Epoch [980], val_loss: 12567.2451\n",
            "Epoch [1000], val_loss: 12516.3096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqISBcuSV5rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af79f94c-ec3a-4f54-9c4b-ce79e9cc4457"
      },
      "source": [
        "epochs = 2000\n",
        "lr = 1e-4\n",
        "history5 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 12465.9229\n",
            "Epoch [40], val_loss: 12415.8057\n",
            "Epoch [60], val_loss: 12366.4805\n",
            "Epoch [80], val_loss: 12317.7490\n",
            "Epoch [100], val_loss: 12269.5557\n",
            "Epoch [120], val_loss: 12221.7041\n",
            "Epoch [140], val_loss: 12174.1592\n",
            "Epoch [160], val_loss: 12127.3359\n",
            "Epoch [180], val_loss: 12080.8916\n",
            "Epoch [200], val_loss: 12034.8936\n",
            "Epoch [220], val_loss: 11989.3086\n",
            "Epoch [240], val_loss: 11943.7627\n",
            "Epoch [260], val_loss: 11898.9639\n",
            "Epoch [280], val_loss: 11854.8203\n",
            "Epoch [300], val_loss: 11811.1904\n",
            "Epoch [320], val_loss: 11767.7334\n",
            "Epoch [340], val_loss: 11725.4375\n",
            "Epoch [360], val_loss: 11683.7451\n",
            "Epoch [380], val_loss: 11642.3643\n",
            "Epoch [400], val_loss: 11600.9883\n",
            "Epoch [420], val_loss: 11560.3564\n",
            "Epoch [440], val_loss: 11519.7686\n",
            "Epoch [460], val_loss: 11479.6123\n",
            "Epoch [480], val_loss: 11439.5703\n",
            "Epoch [500], val_loss: 11399.8467\n",
            "Epoch [520], val_loss: 11360.7139\n",
            "Epoch [540], val_loss: 11321.9639\n",
            "Epoch [560], val_loss: 11283.1924\n",
            "Epoch [580], val_loss: 11245.2725\n",
            "Epoch [600], val_loss: 11208.0264\n",
            "Epoch [620], val_loss: 11171.2510\n",
            "Epoch [640], val_loss: 11134.6143\n",
            "Epoch [660], val_loss: 11098.5742\n",
            "Epoch [680], val_loss: 11062.7109\n",
            "Epoch [700], val_loss: 11027.1367\n",
            "Epoch [720], val_loss: 10991.6885\n",
            "Epoch [740], val_loss: 10956.6680\n",
            "Epoch [760], val_loss: 10921.8350\n",
            "Epoch [780], val_loss: 10887.5000\n",
            "Epoch [800], val_loss: 10854.5947\n",
            "Epoch [820], val_loss: 10822.2100\n",
            "Epoch [840], val_loss: 10789.9004\n",
            "Epoch [860], val_loss: 10758.6182\n",
            "Epoch [880], val_loss: 10727.5830\n",
            "Epoch [900], val_loss: 10697.0400\n",
            "Epoch [920], val_loss: 10666.5928\n",
            "Epoch [940], val_loss: 10636.9297\n",
            "Epoch [960], val_loss: 10607.3311\n",
            "Epoch [980], val_loss: 10578.2002\n",
            "Epoch [1000], val_loss: 10549.4619\n",
            "Epoch [1020], val_loss: 10521.3467\n",
            "Epoch [1040], val_loss: 10493.1914\n",
            "Epoch [1060], val_loss: 10465.5693\n",
            "Epoch [1080], val_loss: 10437.9990\n",
            "Epoch [1100], val_loss: 10410.6729\n",
            "Epoch [1120], val_loss: 10383.8467\n",
            "Epoch [1140], val_loss: 10357.6572\n",
            "Epoch [1160], val_loss: 10331.7002\n",
            "Epoch [1180], val_loss: 10306.2354\n",
            "Epoch [1200], val_loss: 10280.7588\n",
            "Epoch [1220], val_loss: 10255.8467\n",
            "Epoch [1240], val_loss: 10231.6143\n",
            "Epoch [1260], val_loss: 10207.4873\n",
            "Epoch [1280], val_loss: 10183.5547\n",
            "Epoch [1300], val_loss: 10159.7256\n",
            "Epoch [1320], val_loss: 10136.8271\n",
            "Epoch [1340], val_loss: 10113.8711\n",
            "Epoch [1360], val_loss: 10091.4600\n",
            "Epoch [1380], val_loss: 10069.0342\n",
            "Epoch [1400], val_loss: 10047.0303\n",
            "Epoch [1420], val_loss: 10025.7998\n",
            "Epoch [1440], val_loss: 10004.6689\n",
            "Epoch [1460], val_loss: 9983.7783\n",
            "Epoch [1480], val_loss: 9963.5127\n",
            "Epoch [1500], val_loss: 9943.5117\n",
            "Epoch [1520], val_loss: 9924.4199\n",
            "Epoch [1540], val_loss: 9905.9580\n",
            "Epoch [1560], val_loss: 9887.8516\n",
            "Epoch [1580], val_loss: 9869.8525\n",
            "Epoch [1600], val_loss: 9852.0439\n",
            "Epoch [1620], val_loss: 9834.4268\n",
            "Epoch [1640], val_loss: 9817.3477\n",
            "Epoch [1660], val_loss: 9800.4580\n",
            "Epoch [1680], val_loss: 9783.6162\n",
            "Epoch [1700], val_loss: 9767.2432\n",
            "Epoch [1720], val_loss: 9751.4463\n",
            "Epoch [1740], val_loss: 9735.6367\n",
            "Epoch [1760], val_loss: 9720.0078\n",
            "Epoch [1780], val_loss: 9704.7725\n",
            "Epoch [1800], val_loss: 9689.6016\n",
            "Epoch [1820], val_loss: 9674.6133\n",
            "Epoch [1840], val_loss: 9659.6406\n",
            "Epoch [1860], val_loss: 9644.9912\n",
            "Epoch [1880], val_loss: 9630.3740\n",
            "Epoch [1900], val_loss: 9615.8262\n",
            "Epoch [1920], val_loss: 9601.3623\n",
            "Epoch [1940], val_loss: 9586.9990\n",
            "Epoch [1960], val_loss: 9573.1572\n",
            "Epoch [1980], val_loss: 9559.3252\n",
            "Epoch [2000], val_loss: 9545.8125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrTIV7WcZ_t5",
        "outputId": "7ad99efa-81c2-4d90-f1ee-f905975d9dcd"
      },
      "source": [
        "epochs = 4000\r\n",
        "lr = 1e-4\r\n",
        "history5 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 9532.3799\n",
            "Epoch [40], val_loss: 9519.4658\n",
            "Epoch [60], val_loss: 9506.7031\n",
            "Epoch [80], val_loss: 9493.8252\n",
            "Epoch [100], val_loss: 9481.5713\n",
            "Epoch [120], val_loss: 9469.8916\n",
            "Epoch [140], val_loss: 9458.4785\n",
            "Epoch [160], val_loss: 9447.1104\n",
            "Epoch [180], val_loss: 9436.2217\n",
            "Epoch [200], val_loss: 9425.7812\n",
            "Epoch [220], val_loss: 9415.5283\n",
            "Epoch [240], val_loss: 9405.3555\n",
            "Epoch [260], val_loss: 9395.3457\n",
            "Epoch [280], val_loss: 9385.4873\n",
            "Epoch [300], val_loss: 9375.4697\n",
            "Epoch [320], val_loss: 9365.7275\n",
            "Epoch [340], val_loss: 9356.3984\n",
            "Epoch [360], val_loss: 9347.1504\n",
            "Epoch [380], val_loss: 9337.9375\n",
            "Epoch [400], val_loss: 9328.8623\n",
            "Epoch [420], val_loss: 9319.8604\n",
            "Epoch [440], val_loss: 9311.1338\n",
            "Epoch [460], val_loss: 9302.5576\n",
            "Epoch [480], val_loss: 9294.1240\n",
            "Epoch [500], val_loss: 9285.7080\n",
            "Epoch [520], val_loss: 9277.6963\n",
            "Epoch [540], val_loss: 9269.9619\n",
            "Epoch [560], val_loss: 9262.5215\n",
            "Epoch [580], val_loss: 9255.3135\n",
            "Epoch [600], val_loss: 9248.0830\n",
            "Epoch [620], val_loss: 9241.0117\n",
            "Epoch [640], val_loss: 9234.1641\n",
            "Epoch [660], val_loss: 9227.2686\n",
            "Epoch [680], val_loss: 9220.5635\n",
            "Epoch [700], val_loss: 9213.8438\n",
            "Epoch [720], val_loss: 9207.4219\n",
            "Epoch [740], val_loss: 9201.1533\n",
            "Epoch [760], val_loss: 9194.9541\n",
            "Epoch [780], val_loss: 9188.9775\n",
            "Epoch [800], val_loss: 9182.8135\n",
            "Epoch [820], val_loss: 9176.7568\n",
            "Epoch [840], val_loss: 9170.7422\n",
            "Epoch [860], val_loss: 9164.8867\n",
            "Epoch [880], val_loss: 9159.3135\n",
            "Epoch [900], val_loss: 9153.6953\n",
            "Epoch [920], val_loss: 9148.1221\n",
            "Epoch [940], val_loss: 9142.7119\n",
            "Epoch [960], val_loss: 9137.3906\n",
            "Epoch [980], val_loss: 9132.1143\n",
            "Epoch [1000], val_loss: 9126.7852\n",
            "Epoch [1020], val_loss: 9121.4619\n",
            "Epoch [1040], val_loss: 9116.1689\n",
            "Epoch [1060], val_loss: 9111.0391\n",
            "Epoch [1080], val_loss: 9105.8301\n",
            "Epoch [1100], val_loss: 9100.7861\n",
            "Epoch [1120], val_loss: 9095.8438\n",
            "Epoch [1140], val_loss: 9090.9990\n",
            "Epoch [1160], val_loss: 9086.3955\n",
            "Epoch [1180], val_loss: 9082.0420\n",
            "Epoch [1200], val_loss: 9077.6689\n",
            "Epoch [1220], val_loss: 9073.3447\n",
            "Epoch [1240], val_loss: 9069.1240\n",
            "Epoch [1260], val_loss: 9065.0322\n",
            "Epoch [1280], val_loss: 9060.8994\n",
            "Epoch [1300], val_loss: 9056.8477\n",
            "Epoch [1320], val_loss: 9052.9912\n",
            "Epoch [1340], val_loss: 9049.2852\n",
            "Epoch [1360], val_loss: 9045.6279\n",
            "Epoch [1380], val_loss: 9041.9033\n",
            "Epoch [1400], val_loss: 9038.2119\n",
            "Epoch [1420], val_loss: 9034.7646\n",
            "Epoch [1440], val_loss: 9031.4580\n",
            "Epoch [1460], val_loss: 9028.2158\n",
            "Epoch [1480], val_loss: 9024.9951\n",
            "Epoch [1500], val_loss: 9021.8926\n",
            "Epoch [1520], val_loss: 9018.8740\n",
            "Epoch [1540], val_loss: 9015.9873\n",
            "Epoch [1560], val_loss: 9013.1055\n",
            "Epoch [1580], val_loss: 9010.2539\n",
            "Epoch [1600], val_loss: 9007.4932\n",
            "Epoch [1620], val_loss: 9005.0049\n",
            "Epoch [1640], val_loss: 9002.4570\n",
            "Epoch [1660], val_loss: 8999.9287\n",
            "Epoch [1680], val_loss: 8997.4268\n",
            "Epoch [1700], val_loss: 8994.9355\n",
            "Epoch [1720], val_loss: 8992.4854\n",
            "Epoch [1740], val_loss: 8990.0654\n",
            "Epoch [1760], val_loss: 8987.7100\n",
            "Epoch [1780], val_loss: 8985.4092\n",
            "Epoch [1800], val_loss: 8983.2080\n",
            "Epoch [1820], val_loss: 8981.0156\n",
            "Epoch [1840], val_loss: 8978.8350\n",
            "Epoch [1860], val_loss: 8976.7227\n",
            "Epoch [1880], val_loss: 8974.6992\n",
            "Epoch [1900], val_loss: 8972.8027\n",
            "Epoch [1920], val_loss: 8970.8799\n",
            "Epoch [1940], val_loss: 8968.9521\n",
            "Epoch [1960], val_loss: 8967.0205\n",
            "Epoch [1980], val_loss: 8965.1250\n",
            "Epoch [2000], val_loss: 8963.3779\n",
            "Epoch [2020], val_loss: 8961.7588\n",
            "Epoch [2040], val_loss: 8960.1318\n",
            "Epoch [2060], val_loss: 8958.4561\n",
            "Epoch [2080], val_loss: 8956.8193\n",
            "Epoch [2100], val_loss: 8955.2266\n",
            "Epoch [2120], val_loss: 8953.6924\n",
            "Epoch [2140], val_loss: 8952.2334\n",
            "Epoch [2160], val_loss: 8950.9062\n",
            "Epoch [2180], val_loss: 8949.6201\n",
            "Epoch [2200], val_loss: 8948.3447\n",
            "Epoch [2220], val_loss: 8947.0713\n",
            "Epoch [2240], val_loss: 8945.8379\n",
            "Epoch [2260], val_loss: 8944.5332\n",
            "Epoch [2280], val_loss: 8943.3164\n",
            "Epoch [2300], val_loss: 8942.1201\n",
            "Epoch [2320], val_loss: 8941.0439\n",
            "Epoch [2340], val_loss: 8940.0000\n",
            "Epoch [2360], val_loss: 8938.9541\n",
            "Epoch [2380], val_loss: 8937.9414\n",
            "Epoch [2400], val_loss: 8936.9307\n",
            "Epoch [2420], val_loss: 8935.9854\n",
            "Epoch [2440], val_loss: 8935.0498\n",
            "Epoch [2460], val_loss: 8934.1006\n",
            "Epoch [2480], val_loss: 8933.1826\n",
            "Epoch [2500], val_loss: 8932.2373\n",
            "Epoch [2520], val_loss: 8931.2803\n",
            "Epoch [2540], val_loss: 8930.3555\n",
            "Epoch [2560], val_loss: 8929.4453\n",
            "Epoch [2580], val_loss: 8928.5225\n",
            "Epoch [2600], val_loss: 8927.6182\n",
            "Epoch [2620], val_loss: 8926.7275\n",
            "Epoch [2640], val_loss: 8925.8232\n",
            "Epoch [2660], val_loss: 8924.9336\n",
            "Epoch [2680], val_loss: 8924.0459\n",
            "Epoch [2700], val_loss: 8923.1318\n",
            "Epoch [2720], val_loss: 8922.2061\n",
            "Epoch [2740], val_loss: 8921.3193\n",
            "Epoch [2760], val_loss: 8920.4307\n",
            "Epoch [2780], val_loss: 8919.5557\n",
            "Epoch [2800], val_loss: 8918.7188\n",
            "Epoch [2820], val_loss: 8917.8525\n",
            "Epoch [2840], val_loss: 8916.9814\n",
            "Epoch [2860], val_loss: 8916.1084\n",
            "Epoch [2880], val_loss: 8915.2734\n",
            "Epoch [2900], val_loss: 8914.4307\n",
            "Epoch [2920], val_loss: 8913.5586\n",
            "Epoch [2940], val_loss: 8912.7295\n",
            "Epoch [2960], val_loss: 8911.8975\n",
            "Epoch [2980], val_loss: 8911.0947\n",
            "Epoch [3000], val_loss: 8910.3018\n",
            "Epoch [3020], val_loss: 8909.4873\n",
            "Epoch [3040], val_loss: 8908.6875\n",
            "Epoch [3060], val_loss: 8907.9336\n",
            "Epoch [3080], val_loss: 8907.1709\n",
            "Epoch [3100], val_loss: 8906.4014\n",
            "Epoch [3120], val_loss: 8905.6299\n",
            "Epoch [3140], val_loss: 8904.8887\n",
            "Epoch [3160], val_loss: 8904.1494\n",
            "Epoch [3180], val_loss: 8903.4219\n",
            "Epoch [3200], val_loss: 8902.6748\n",
            "Epoch [3220], val_loss: 8901.9287\n",
            "Epoch [3240], val_loss: 8901.2178\n",
            "Epoch [3260], val_loss: 8900.4619\n",
            "Epoch [3280], val_loss: 8899.7305\n",
            "Epoch [3300], val_loss: 8899.0176\n",
            "Epoch [3320], val_loss: 8898.2764\n",
            "Epoch [3340], val_loss: 8897.5479\n",
            "Epoch [3360], val_loss: 8896.8789\n",
            "Epoch [3380], val_loss: 8896.2109\n",
            "Epoch [3400], val_loss: 8895.5352\n",
            "Epoch [3420], val_loss: 8894.8926\n",
            "Epoch [3440], val_loss: 8894.2256\n",
            "Epoch [3460], val_loss: 8893.5654\n",
            "Epoch [3480], val_loss: 8892.9072\n",
            "Epoch [3500], val_loss: 8892.2354\n",
            "Epoch [3520], val_loss: 8891.5967\n",
            "Epoch [3540], val_loss: 8890.9873\n",
            "Epoch [3560], val_loss: 8890.3369\n",
            "Epoch [3580], val_loss: 8889.6904\n",
            "Epoch [3600], val_loss: 8889.0498\n",
            "Epoch [3620], val_loss: 8888.3896\n",
            "Epoch [3640], val_loss: 8887.7256\n",
            "Epoch [3660], val_loss: 8887.0693\n",
            "Epoch [3680], val_loss: 8886.4229\n",
            "Epoch [3700], val_loss: 8885.7891\n",
            "Epoch [3720], val_loss: 8885.1367\n",
            "Epoch [3740], val_loss: 8884.4854\n",
            "Epoch [3760], val_loss: 8883.8154\n",
            "Epoch [3780], val_loss: 8883.1748\n",
            "Epoch [3800], val_loss: 8882.5225\n",
            "Epoch [3820], val_loss: 8881.8838\n",
            "Epoch [3840], val_loss: 8881.2881\n",
            "Epoch [3860], val_loss: 8880.6729\n",
            "Epoch [3880], val_loss: 8880.0576\n",
            "Epoch [3900], val_loss: 8879.4404\n",
            "Epoch [3920], val_loss: 8878.8320\n",
            "Epoch [3940], val_loss: 8878.2480\n",
            "Epoch [3960], val_loss: 8877.6494\n",
            "Epoch [3980], val_loss: 8877.0566\n",
            "Epoch [4000], val_loss: 8876.4746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UY638rPaYkc",
        "outputId": "8a22be40-9395-4890-d967-21ec76043d27"
      },
      "source": [
        "epochs = 4000\r\n",
        "lr = 1e-3\r\n",
        "history5 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 8870.7305\n",
            "Epoch [40], val_loss: 8865.4189\n",
            "Epoch [60], val_loss: 8860.0205\n",
            "Epoch [80], val_loss: 8855.1494\n",
            "Epoch [100], val_loss: 8850.3721\n",
            "Epoch [120], val_loss: 8845.9287\n",
            "Epoch [140], val_loss: 8841.5088\n",
            "Epoch [160], val_loss: 8837.1807\n",
            "Epoch [180], val_loss: 8832.8623\n",
            "Epoch [200], val_loss: 8828.6523\n",
            "Epoch [220], val_loss: 8824.6084\n",
            "Epoch [240], val_loss: 8820.5898\n",
            "Epoch [260], val_loss: 8816.6035\n",
            "Epoch [280], val_loss: 8812.6572\n",
            "Epoch [300], val_loss: 8808.6211\n",
            "Epoch [320], val_loss: 8804.6191\n",
            "Epoch [340], val_loss: 8800.5625\n",
            "Epoch [360], val_loss: 8796.4346\n",
            "Epoch [380], val_loss: 8792.4453\n",
            "Epoch [400], val_loss: 8788.4795\n",
            "Epoch [420], val_loss: 8784.3096\n",
            "Epoch [440], val_loss: 8780.3193\n",
            "Epoch [460], val_loss: 8776.4170\n",
            "Epoch [480], val_loss: 8772.4248\n",
            "Epoch [500], val_loss: 8768.4004\n",
            "Epoch [520], val_loss: 8764.5371\n",
            "Epoch [540], val_loss: 8760.6377\n",
            "Epoch [560], val_loss: 8756.7002\n",
            "Epoch [580], val_loss: 8752.8438\n",
            "Epoch [600], val_loss: 8748.8955\n",
            "Epoch [620], val_loss: 8744.9834\n",
            "Epoch [640], val_loss: 8741.1279\n",
            "Epoch [660], val_loss: 8737.1377\n",
            "Epoch [680], val_loss: 8733.1729\n",
            "Epoch [700], val_loss: 8729.3711\n",
            "Epoch [720], val_loss: 8725.5205\n",
            "Epoch [740], val_loss: 8721.5283\n",
            "Epoch [760], val_loss: 8717.6709\n",
            "Epoch [780], val_loss: 8713.7178\n",
            "Epoch [800], val_loss: 8709.7412\n",
            "Epoch [820], val_loss: 8705.7334\n",
            "Epoch [840], val_loss: 8701.7783\n",
            "Epoch [860], val_loss: 8697.8408\n",
            "Epoch [880], val_loss: 8693.8760\n",
            "Epoch [900], val_loss: 8689.9229\n",
            "Epoch [920], val_loss: 8686.0967\n",
            "Epoch [940], val_loss: 8682.1328\n",
            "Epoch [960], val_loss: 8678.1865\n",
            "Epoch [980], val_loss: 8674.2666\n",
            "Epoch [1000], val_loss: 8670.4033\n",
            "Epoch [1020], val_loss: 8666.5068\n",
            "Epoch [1040], val_loss: 8662.5225\n",
            "Epoch [1060], val_loss: 8658.6455\n",
            "Epoch [1080], val_loss: 8654.7266\n",
            "Epoch [1100], val_loss: 8650.9502\n",
            "Epoch [1120], val_loss: 8647.1602\n",
            "Epoch [1140], val_loss: 8643.2744\n",
            "Epoch [1160], val_loss: 8639.3486\n",
            "Epoch [1180], val_loss: 8635.5322\n",
            "Epoch [1200], val_loss: 8631.8154\n",
            "Epoch [1220], val_loss: 8628.2334\n",
            "Epoch [1240], val_loss: 8624.6729\n",
            "Epoch [1260], val_loss: 8621.1602\n",
            "Epoch [1280], val_loss: 8617.5967\n",
            "Epoch [1300], val_loss: 8613.9482\n",
            "Epoch [1320], val_loss: 8610.1260\n",
            "Epoch [1340], val_loss: 8606.5352\n",
            "Epoch [1360], val_loss: 8602.8252\n",
            "Epoch [1380], val_loss: 8599.2100\n",
            "Epoch [1400], val_loss: 8595.4482\n",
            "Epoch [1420], val_loss: 8591.7490\n",
            "Epoch [1440], val_loss: 8588.1172\n",
            "Epoch [1460], val_loss: 8584.4189\n",
            "Epoch [1480], val_loss: 8580.7939\n",
            "Epoch [1500], val_loss: 8577.4229\n",
            "Epoch [1520], val_loss: 8573.8643\n",
            "Epoch [1540], val_loss: 8570.4658\n",
            "Epoch [1560], val_loss: 8566.8750\n",
            "Epoch [1580], val_loss: 8563.2197\n",
            "Epoch [1600], val_loss: 8559.7080\n",
            "Epoch [1620], val_loss: 8556.2930\n",
            "Epoch [1640], val_loss: 8552.8291\n",
            "Epoch [1660], val_loss: 8549.6602\n",
            "Epoch [1680], val_loss: 8546.2852\n",
            "Epoch [1700], val_loss: 8542.9941\n",
            "Epoch [1720], val_loss: 8539.7705\n",
            "Epoch [1740], val_loss: 8536.3799\n",
            "Epoch [1760], val_loss: 8533.2197\n",
            "Epoch [1780], val_loss: 8530.1240\n",
            "Epoch [1800], val_loss: 8526.9326\n",
            "Epoch [1820], val_loss: 8523.7510\n",
            "Epoch [1840], val_loss: 8520.5146\n",
            "Epoch [1860], val_loss: 8517.2080\n",
            "Epoch [1880], val_loss: 8513.9189\n",
            "Epoch [1900], val_loss: 8510.4590\n",
            "Epoch [1920], val_loss: 8507.1709\n",
            "Epoch [1940], val_loss: 8503.8770\n",
            "Epoch [1960], val_loss: 8500.6914\n",
            "Epoch [1980], val_loss: 8497.4287\n",
            "Epoch [2000], val_loss: 8494.3271\n",
            "Epoch [2020], val_loss: 8490.9922\n",
            "Epoch [2040], val_loss: 8487.7373\n",
            "Epoch [2060], val_loss: 8484.5537\n",
            "Epoch [2080], val_loss: 8481.2295\n",
            "Epoch [2100], val_loss: 8478.2461\n",
            "Epoch [2120], val_loss: 8475.0166\n",
            "Epoch [2140], val_loss: 8471.7617\n",
            "Epoch [2160], val_loss: 8468.8018\n",
            "Epoch [2180], val_loss: 8465.7334\n",
            "Epoch [2200], val_loss: 8462.5947\n",
            "Epoch [2220], val_loss: 8459.3975\n",
            "Epoch [2240], val_loss: 8456.3896\n",
            "Epoch [2260], val_loss: 8453.3545\n",
            "Epoch [2280], val_loss: 8450.3662\n",
            "Epoch [2300], val_loss: 8447.4102\n",
            "Epoch [2320], val_loss: 8444.4150\n",
            "Epoch [2340], val_loss: 8441.3604\n",
            "Epoch [2360], val_loss: 8438.1299\n",
            "Epoch [2380], val_loss: 8435.0176\n",
            "Epoch [2400], val_loss: 8431.9932\n",
            "Epoch [2420], val_loss: 8428.8291\n",
            "Epoch [2440], val_loss: 8425.8193\n",
            "Epoch [2460], val_loss: 8423.0459\n",
            "Epoch [2480], val_loss: 8419.9717\n",
            "Epoch [2500], val_loss: 8416.7842\n",
            "Epoch [2520], val_loss: 8413.8242\n",
            "Epoch [2540], val_loss: 8410.7686\n",
            "Epoch [2560], val_loss: 8407.8721\n",
            "Epoch [2580], val_loss: 8404.6680\n",
            "Epoch [2600], val_loss: 8401.5537\n",
            "Epoch [2620], val_loss: 8398.3877\n",
            "Epoch [2640], val_loss: 8395.2148\n",
            "Epoch [2660], val_loss: 8392.3164\n",
            "Epoch [2680], val_loss: 8389.1807\n",
            "Epoch [2700], val_loss: 8386.2412\n",
            "Epoch [2720], val_loss: 8383.3398\n",
            "Epoch [2740], val_loss: 8380.3740\n",
            "Epoch [2760], val_loss: 8377.3760\n",
            "Epoch [2780], val_loss: 8374.2314\n",
            "Epoch [2800], val_loss: 8371.1211\n",
            "Epoch [2820], val_loss: 8368.2334\n",
            "Epoch [2840], val_loss: 8365.2764\n",
            "Epoch [2860], val_loss: 8362.1104\n",
            "Epoch [2880], val_loss: 8359.4482\n",
            "Epoch [2900], val_loss: 8356.4482\n",
            "Epoch [2920], val_loss: 8353.6953\n",
            "Epoch [2940], val_loss: 8351.1201\n",
            "Epoch [2960], val_loss: 8348.5498\n",
            "Epoch [2980], val_loss: 8345.6416\n",
            "Epoch [3000], val_loss: 8342.8643\n",
            "Epoch [3020], val_loss: 8340.1172\n",
            "Epoch [3040], val_loss: 8337.3662\n",
            "Epoch [3060], val_loss: 8334.6123\n",
            "Epoch [3080], val_loss: 8331.9072\n",
            "Epoch [3100], val_loss: 8329.2666\n",
            "Epoch [3120], val_loss: 8326.6279\n",
            "Epoch [3140], val_loss: 8323.8857\n",
            "Epoch [3160], val_loss: 8321.4365\n",
            "Epoch [3180], val_loss: 8318.9062\n",
            "Epoch [3200], val_loss: 8316.4463\n",
            "Epoch [3220], val_loss: 8313.8203\n",
            "Epoch [3240], val_loss: 8311.3467\n",
            "Epoch [3260], val_loss: 8308.9521\n",
            "Epoch [3280], val_loss: 8305.9189\n",
            "Epoch [3300], val_loss: 8303.3613\n",
            "Epoch [3320], val_loss: 8300.8252\n",
            "Epoch [3340], val_loss: 8298.5000\n",
            "Epoch [3360], val_loss: 8296.4072\n",
            "Epoch [3380], val_loss: 8294.3516\n",
            "Epoch [3400], val_loss: 8292.2646\n",
            "Epoch [3420], val_loss: 8290.2539\n",
            "Epoch [3440], val_loss: 8288.3281\n",
            "Epoch [3460], val_loss: 8286.4580\n",
            "Epoch [3480], val_loss: 8284.5596\n",
            "Epoch [3500], val_loss: 8282.7295\n",
            "Epoch [3520], val_loss: 8280.8418\n",
            "Epoch [3540], val_loss: 8279.0732\n",
            "Epoch [3560], val_loss: 8277.2725\n",
            "Epoch [3580], val_loss: 8275.4541\n",
            "Epoch [3600], val_loss: 8273.7686\n",
            "Epoch [3620], val_loss: 8272.0908\n",
            "Epoch [3640], val_loss: 8270.3545\n",
            "Epoch [3660], val_loss: 8268.6289\n",
            "Epoch [3680], val_loss: 8266.9453\n",
            "Epoch [3700], val_loss: 8265.2334\n",
            "Epoch [3720], val_loss: 8263.4424\n",
            "Epoch [3740], val_loss: 8261.6191\n",
            "Epoch [3760], val_loss: 8259.9258\n",
            "Epoch [3780], val_loss: 8258.2549\n",
            "Epoch [3800], val_loss: 8256.4766\n",
            "Epoch [3820], val_loss: 8254.8047\n",
            "Epoch [3840], val_loss: 8253.1338\n",
            "Epoch [3860], val_loss: 8251.4990\n",
            "Epoch [3880], val_loss: 8249.8984\n",
            "Epoch [3900], val_loss: 8248.3193\n",
            "Epoch [3920], val_loss: 8246.7764\n",
            "Epoch [3940], val_loss: 8245.2646\n",
            "Epoch [3960], val_loss: 8243.8184\n",
            "Epoch [3980], val_loss: 8242.2100\n",
            "Epoch [4000], val_loss: 8240.6758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBWOIbcua7Dp",
        "outputId": "cb803e22-ed16-4add-bbea-26f7479e1940"
      },
      "source": [
        "epochs = 6000\r\n",
        "lr = 1e-3\r\n",
        "history5 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 8239.1562\n",
            "Epoch [40], val_loss: 8237.5693\n",
            "Epoch [60], val_loss: 8235.9453\n",
            "Epoch [80], val_loss: 8234.3818\n",
            "Epoch [100], val_loss: 8232.8174\n",
            "Epoch [120], val_loss: 8231.2666\n",
            "Epoch [140], val_loss: 8229.7236\n",
            "Epoch [160], val_loss: 8228.2295\n",
            "Epoch [180], val_loss: 8226.7695\n",
            "Epoch [200], val_loss: 8225.2930\n",
            "Epoch [220], val_loss: 8223.8467\n",
            "Epoch [240], val_loss: 8222.4023\n",
            "Epoch [260], val_loss: 8220.9814\n",
            "Epoch [280], val_loss: 8219.6729\n",
            "Epoch [300], val_loss: 8218.3281\n",
            "Epoch [320], val_loss: 8216.8994\n",
            "Epoch [340], val_loss: 8215.6064\n",
            "Epoch [360], val_loss: 8214.3818\n",
            "Epoch [380], val_loss: 8213.2559\n",
            "Epoch [400], val_loss: 8212.0400\n",
            "Epoch [420], val_loss: 8210.9336\n",
            "Epoch [440], val_loss: 8209.8232\n",
            "Epoch [460], val_loss: 8208.4268\n",
            "Epoch [480], val_loss: 8207.2393\n",
            "Epoch [500], val_loss: 8206.2041\n",
            "Epoch [520], val_loss: 8205.0068\n",
            "Epoch [540], val_loss: 8203.8311\n",
            "Epoch [560], val_loss: 8202.7236\n",
            "Epoch [580], val_loss: 8201.7666\n",
            "Epoch [600], val_loss: 8200.9971\n",
            "Epoch [620], val_loss: 8200.0068\n",
            "Epoch [640], val_loss: 8198.9893\n",
            "Epoch [660], val_loss: 8198.1162\n",
            "Epoch [680], val_loss: 8197.1914\n",
            "Epoch [700], val_loss: 8196.2305\n",
            "Epoch [720], val_loss: 8195.5039\n",
            "Epoch [740], val_loss: 8194.5820\n",
            "Epoch [760], val_loss: 8193.5752\n",
            "Epoch [780], val_loss: 8192.7080\n",
            "Epoch [800], val_loss: 8191.9087\n",
            "Epoch [820], val_loss: 8191.1133\n",
            "Epoch [840], val_loss: 8190.2017\n",
            "Epoch [860], val_loss: 8189.3638\n",
            "Epoch [880], val_loss: 8188.4414\n",
            "Epoch [900], val_loss: 8187.6509\n",
            "Epoch [920], val_loss: 8186.8828\n",
            "Epoch [940], val_loss: 8186.0151\n",
            "Epoch [960], val_loss: 8185.1289\n",
            "Epoch [980], val_loss: 8184.2876\n",
            "Epoch [1000], val_loss: 8183.4272\n",
            "Epoch [1020], val_loss: 8182.6851\n",
            "Epoch [1040], val_loss: 8182.0679\n",
            "Epoch [1060], val_loss: 8181.3267\n",
            "Epoch [1080], val_loss: 8180.5605\n",
            "Epoch [1100], val_loss: 8179.8950\n",
            "Epoch [1120], val_loss: 8179.1523\n",
            "Epoch [1140], val_loss: 8178.4180\n",
            "Epoch [1160], val_loss: 8177.7734\n",
            "Epoch [1180], val_loss: 8177.0581\n",
            "Epoch [1200], val_loss: 8176.3140\n",
            "Epoch [1220], val_loss: 8175.6304\n",
            "Epoch [1240], val_loss: 8175.0259\n",
            "Epoch [1260], val_loss: 8174.3945\n",
            "Epoch [1280], val_loss: 8173.8008\n",
            "Epoch [1300], val_loss: 8173.0161\n",
            "Epoch [1320], val_loss: 8172.4976\n",
            "Epoch [1340], val_loss: 8171.8105\n",
            "Epoch [1360], val_loss: 8171.1763\n",
            "Epoch [1380], val_loss: 8170.5972\n",
            "Epoch [1400], val_loss: 8169.9883\n",
            "Epoch [1420], val_loss: 8169.5571\n",
            "Epoch [1440], val_loss: 8169.0796\n",
            "Epoch [1460], val_loss: 8168.3970\n",
            "Epoch [1480], val_loss: 8167.8804\n",
            "Epoch [1500], val_loss: 8167.4492\n",
            "Epoch [1520], val_loss: 8167.0181\n",
            "Epoch [1540], val_loss: 8166.6099\n",
            "Epoch [1560], val_loss: 8166.2554\n",
            "Epoch [1580], val_loss: 8165.7944\n",
            "Epoch [1600], val_loss: 8165.4531\n",
            "Epoch [1620], val_loss: 8165.0117\n",
            "Epoch [1640], val_loss: 8164.5796\n",
            "Epoch [1660], val_loss: 8164.0093\n",
            "Epoch [1680], val_loss: 8163.7505\n",
            "Epoch [1700], val_loss: 8163.4102\n",
            "Epoch [1720], val_loss: 8163.1353\n",
            "Epoch [1740], val_loss: 8162.8335\n",
            "Epoch [1760], val_loss: 8162.2944\n",
            "Epoch [1780], val_loss: 8162.0430\n",
            "Epoch [1800], val_loss: 8161.7617\n",
            "Epoch [1820], val_loss: 8161.4780\n",
            "Epoch [1840], val_loss: 8161.1392\n",
            "Epoch [1860], val_loss: 8160.8882\n",
            "Epoch [1880], val_loss: 8160.6914\n",
            "Epoch [1900], val_loss: 8160.3657\n",
            "Epoch [1920], val_loss: 8159.9624\n",
            "Epoch [1940], val_loss: 8159.7422\n",
            "Epoch [1960], val_loss: 8159.7407\n",
            "Epoch [1980], val_loss: 8159.6187\n",
            "Epoch [2000], val_loss: 8159.4648\n",
            "Epoch [2020], val_loss: 8159.3828\n",
            "Epoch [2040], val_loss: 8159.0386\n",
            "Epoch [2060], val_loss: 8158.7495\n",
            "Epoch [2080], val_loss: 8158.3750\n",
            "Epoch [2100], val_loss: 8158.0112\n",
            "Epoch [2120], val_loss: 8157.8242\n",
            "Epoch [2140], val_loss: 8157.5312\n",
            "Epoch [2160], val_loss: 8156.9282\n",
            "Epoch [2180], val_loss: 8156.7114\n",
            "Epoch [2200], val_loss: 8156.4321\n",
            "Epoch [2220], val_loss: 8155.9712\n",
            "Epoch [2240], val_loss: 8155.3999\n",
            "Epoch [2260], val_loss: 8155.0132\n",
            "Epoch [2280], val_loss: 8154.8696\n",
            "Epoch [2300], val_loss: 8154.9360\n",
            "Epoch [2320], val_loss: 8154.7734\n",
            "Epoch [2340], val_loss: 8154.5649\n",
            "Epoch [2360], val_loss: 8154.1792\n",
            "Epoch [2380], val_loss: 8153.9780\n",
            "Epoch [2400], val_loss: 8153.8120\n",
            "Epoch [2420], val_loss: 8153.4487\n",
            "Epoch [2440], val_loss: 8153.3047\n",
            "Epoch [2460], val_loss: 8153.0142\n",
            "Epoch [2480], val_loss: 8152.8652\n",
            "Epoch [2500], val_loss: 8152.5288\n",
            "Epoch [2520], val_loss: 8152.4478\n",
            "Epoch [2540], val_loss: 8152.0098\n",
            "Epoch [2560], val_loss: 8151.6509\n",
            "Epoch [2580], val_loss: 8151.3315\n",
            "Epoch [2600], val_loss: 8150.9751\n",
            "Epoch [2620], val_loss: 8150.8574\n",
            "Epoch [2640], val_loss: 8150.4907\n",
            "Epoch [2660], val_loss: 8150.1001\n",
            "Epoch [2680], val_loss: 8149.9360\n",
            "Epoch [2700], val_loss: 8149.8164\n",
            "Epoch [2720], val_loss: 8149.3394\n",
            "Epoch [2740], val_loss: 8149.3462\n",
            "Epoch [2760], val_loss: 8149.0649\n",
            "Epoch [2780], val_loss: 8148.9321\n",
            "Epoch [2800], val_loss: 8148.8613\n",
            "Epoch [2820], val_loss: 8148.8384\n",
            "Epoch [2840], val_loss: 8148.5742\n",
            "Epoch [2860], val_loss: 8148.4819\n",
            "Epoch [2880], val_loss: 8148.4399\n",
            "Epoch [2900], val_loss: 8148.2070\n",
            "Epoch [2920], val_loss: 8147.9155\n",
            "Epoch [2940], val_loss: 8147.7241\n",
            "Epoch [2960], val_loss: 8147.5542\n",
            "Epoch [2980], val_loss: 8147.4219\n",
            "Epoch [3000], val_loss: 8147.3125\n",
            "Epoch [3020], val_loss: 8147.1875\n",
            "Epoch [3040], val_loss: 8147.0503\n",
            "Epoch [3060], val_loss: 8146.9961\n",
            "Epoch [3080], val_loss: 8146.8906\n",
            "Epoch [3100], val_loss: 8146.7681\n",
            "Epoch [3120], val_loss: 8146.4609\n",
            "Epoch [3140], val_loss: 8146.3853\n",
            "Epoch [3160], val_loss: 8146.2954\n",
            "Epoch [3180], val_loss: 8146.2798\n",
            "Epoch [3200], val_loss: 8146.2202\n",
            "Epoch [3220], val_loss: 8146.1328\n",
            "Epoch [3240], val_loss: 8146.0020\n",
            "Epoch [3260], val_loss: 8146.0273\n",
            "Epoch [3280], val_loss: 8146.1445\n",
            "Epoch [3300], val_loss: 8146.1050\n",
            "Epoch [3320], val_loss: 8146.0952\n",
            "Epoch [3340], val_loss: 8145.8931\n",
            "Epoch [3360], val_loss: 8145.9155\n",
            "Epoch [3380], val_loss: 8145.7935\n",
            "Epoch [3400], val_loss: 8145.7319\n",
            "Epoch [3420], val_loss: 8145.7441\n",
            "Epoch [3440], val_loss: 8145.7358\n",
            "Epoch [3460], val_loss: 8145.7339\n",
            "Epoch [3480], val_loss: 8145.6528\n",
            "Epoch [3500], val_loss: 8145.5776\n",
            "Epoch [3520], val_loss: 8145.5317\n",
            "Epoch [3540], val_loss: 8145.4751\n",
            "Epoch [3560], val_loss: 8145.4614\n",
            "Epoch [3580], val_loss: 8145.4883\n",
            "Epoch [3600], val_loss: 8145.4434\n",
            "Epoch [3620], val_loss: 8145.4497\n",
            "Epoch [3640], val_loss: 8145.3804\n",
            "Epoch [3660], val_loss: 8145.3306\n",
            "Epoch [3680], val_loss: 8145.3130\n",
            "Epoch [3700], val_loss: 8145.2632\n",
            "Epoch [3720], val_loss: 8145.2925\n",
            "Epoch [3740], val_loss: 8145.2710\n",
            "Epoch [3760], val_loss: 8145.2524\n",
            "Epoch [3780], val_loss: 8145.2534\n",
            "Epoch [3800], val_loss: 8145.1978\n",
            "Epoch [3820], val_loss: 8145.1958\n",
            "Epoch [3840], val_loss: 8145.1147\n",
            "Epoch [3860], val_loss: 8145.0845\n",
            "Epoch [3880], val_loss: 8145.0796\n",
            "Epoch [3900], val_loss: 8145.0317\n",
            "Epoch [3920], val_loss: 8144.9702\n",
            "Epoch [3940], val_loss: 8144.9878\n",
            "Epoch [3960], val_loss: 8144.9673\n",
            "Epoch [3980], val_loss: 8144.9507\n",
            "Epoch [4000], val_loss: 8144.9009\n",
            "Epoch [4020], val_loss: 8144.8999\n",
            "Epoch [4040], val_loss: 8144.8345\n",
            "Epoch [4060], val_loss: 8144.8125\n",
            "Epoch [4080], val_loss: 8144.8022\n",
            "Epoch [4100], val_loss: 8144.7778\n",
            "Epoch [4120], val_loss: 8144.7944\n",
            "Epoch [4140], val_loss: 8144.7476\n",
            "Epoch [4160], val_loss: 8144.7437\n",
            "Epoch [4180], val_loss: 8144.7383\n",
            "Epoch [4200], val_loss: 8144.7090\n",
            "Epoch [4220], val_loss: 8144.6890\n",
            "Epoch [4240], val_loss: 8144.7109\n",
            "Epoch [4260], val_loss: 8144.6860\n",
            "Epoch [4280], val_loss: 8144.6030\n",
            "Epoch [4300], val_loss: 8144.6001\n",
            "Epoch [4320], val_loss: 8144.5806\n",
            "Epoch [4340], val_loss: 8144.5571\n",
            "Epoch [4360], val_loss: 8144.5039\n",
            "Epoch [4380], val_loss: 8144.4922\n",
            "Epoch [4400], val_loss: 8144.4468\n",
            "Epoch [4420], val_loss: 8144.3882\n",
            "Epoch [4440], val_loss: 8144.3516\n",
            "Epoch [4460], val_loss: 8144.3179\n",
            "Epoch [4480], val_loss: 8144.3237\n",
            "Epoch [4500], val_loss: 8144.2617\n",
            "Epoch [4520], val_loss: 8144.2183\n",
            "Epoch [4540], val_loss: 8144.2046\n",
            "Epoch [4560], val_loss: 8144.2163\n",
            "Epoch [4580], val_loss: 8144.2148\n",
            "Epoch [4600], val_loss: 8144.2114\n",
            "Epoch [4620], val_loss: 8144.2192\n",
            "Epoch [4640], val_loss: 8144.2026\n",
            "Epoch [4660], val_loss: 8144.2085\n",
            "Epoch [4680], val_loss: 8144.1465\n",
            "Epoch [4700], val_loss: 8144.0859\n",
            "Epoch [4720], val_loss: 8144.0649\n",
            "Epoch [4740], val_loss: 8144.0171\n",
            "Epoch [4760], val_loss: 8144.0273\n",
            "Epoch [4780], val_loss: 8143.9976\n",
            "Epoch [4800], val_loss: 8143.9805\n",
            "Epoch [4820], val_loss: 8143.9844\n",
            "Epoch [4840], val_loss: 8143.9492\n",
            "Epoch [4860], val_loss: 8143.9438\n",
            "Epoch [4880], val_loss: 8143.8979\n",
            "Epoch [4900], val_loss: 8143.8071\n",
            "Epoch [4920], val_loss: 8143.7798\n",
            "Epoch [4940], val_loss: 8143.8022\n",
            "Epoch [4960], val_loss: 8143.7319\n",
            "Epoch [4980], val_loss: 8143.7363\n",
            "Epoch [5000], val_loss: 8143.7124\n",
            "Epoch [5020], val_loss: 8143.6953\n",
            "Epoch [5040], val_loss: 8143.6704\n",
            "Epoch [5060], val_loss: 8143.6523\n",
            "Epoch [5080], val_loss: 8143.6362\n",
            "Epoch [5100], val_loss: 8143.6211\n",
            "Epoch [5120], val_loss: 8143.5894\n",
            "Epoch [5140], val_loss: 8143.5508\n",
            "Epoch [5160], val_loss: 8143.5503\n",
            "Epoch [5180], val_loss: 8143.5371\n",
            "Epoch [5200], val_loss: 8143.5249\n",
            "Epoch [5220], val_loss: 8143.5020\n",
            "Epoch [5240], val_loss: 8143.4819\n",
            "Epoch [5260], val_loss: 8143.4609\n",
            "Epoch [5280], val_loss: 8143.4438\n",
            "Epoch [5300], val_loss: 8143.4102\n",
            "Epoch [5320], val_loss: 8143.3999\n",
            "Epoch [5340], val_loss: 8143.3647\n",
            "Epoch [5360], val_loss: 8143.3394\n",
            "Epoch [5380], val_loss: 8143.3296\n",
            "Epoch [5400], val_loss: 8143.3228\n",
            "Epoch [5420], val_loss: 8143.3188\n",
            "Epoch [5440], val_loss: 8143.3032\n",
            "Epoch [5460], val_loss: 8143.2734\n",
            "Epoch [5480], val_loss: 8143.2407\n",
            "Epoch [5500], val_loss: 8143.2305\n",
            "Epoch [5520], val_loss: 8143.2017\n",
            "Epoch [5540], val_loss: 8143.2056\n",
            "Epoch [5560], val_loss: 8143.1890\n",
            "Epoch [5580], val_loss: 8143.1836\n",
            "Epoch [5600], val_loss: 8143.1484\n",
            "Epoch [5620], val_loss: 8143.1304\n",
            "Epoch [5640], val_loss: 8143.1167\n",
            "Epoch [5660], val_loss: 8143.0864\n",
            "Epoch [5680], val_loss: 8143.0840\n",
            "Epoch [5700], val_loss: 8143.0581\n",
            "Epoch [5720], val_loss: 8143.0278\n",
            "Epoch [5740], val_loss: 8142.9888\n",
            "Epoch [5760], val_loss: 8142.9829\n",
            "Epoch [5780], val_loss: 8142.9746\n",
            "Epoch [5800], val_loss: 8142.9507\n",
            "Epoch [5820], val_loss: 8142.9375\n",
            "Epoch [5840], val_loss: 8142.9116\n",
            "Epoch [5860], val_loss: 8142.9165\n",
            "Epoch [5880], val_loss: 8142.8687\n",
            "Epoch [5900], val_loss: 8142.8511\n",
            "Epoch [5920], val_loss: 8142.8306\n",
            "Epoch [5940], val_loss: 8142.8066\n",
            "Epoch [5960], val_loss: 8142.8101\n",
            "Epoch [5980], val_loss: 8142.8296\n",
            "Epoch [6000], val_loss: 8142.8726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qftq2_U1bZcl",
        "outputId": "b23d5cb5-eadc-43cb-ff9e-bc99b0904b84"
      },
      "source": [
        "epochs = 6000\r\n",
        "lr = 1e-2\r\n",
        "history5 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 8142.4727\n",
            "Epoch [40], val_loss: 8142.5220\n",
            "Epoch [60], val_loss: 8142.2632\n",
            "Epoch [80], val_loss: 8141.9961\n",
            "Epoch [100], val_loss: 8141.9927\n",
            "Epoch [120], val_loss: 8141.7793\n",
            "Epoch [140], val_loss: 8141.5645\n",
            "Epoch [160], val_loss: 8141.4321\n",
            "Epoch [180], val_loss: 8141.2539\n",
            "Epoch [200], val_loss: 8141.1812\n",
            "Epoch [220], val_loss: 8141.1309\n",
            "Epoch [240], val_loss: 8140.8296\n",
            "Epoch [260], val_loss: 8140.6587\n",
            "Epoch [280], val_loss: 8140.4961\n",
            "Epoch [300], val_loss: 8140.4375\n",
            "Epoch [320], val_loss: 8140.1797\n",
            "Epoch [340], val_loss: 8140.1138\n",
            "Epoch [360], val_loss: 8139.7695\n",
            "Epoch [380], val_loss: 8139.6187\n",
            "Epoch [400], val_loss: 8139.4077\n",
            "Epoch [420], val_loss: 8139.3032\n",
            "Epoch [440], val_loss: 8139.1113\n",
            "Epoch [460], val_loss: 8138.9395\n",
            "Epoch [480], val_loss: 8138.6851\n",
            "Epoch [500], val_loss: 8138.5229\n",
            "Epoch [520], val_loss: 8138.2964\n",
            "Epoch [540], val_loss: 8138.1792\n",
            "Epoch [560], val_loss: 8138.0063\n",
            "Epoch [580], val_loss: 8137.8203\n",
            "Epoch [600], val_loss: 8137.6992\n",
            "Epoch [620], val_loss: 8137.4927\n",
            "Epoch [640], val_loss: 8137.2788\n",
            "Epoch [660], val_loss: 8137.3047\n",
            "Epoch [680], val_loss: 8136.8867\n",
            "Epoch [700], val_loss: 8136.6875\n",
            "Epoch [720], val_loss: 8136.5522\n",
            "Epoch [740], val_loss: 8136.6694\n",
            "Epoch [760], val_loss: 8136.2222\n",
            "Epoch [780], val_loss: 8136.0562\n",
            "Epoch [800], val_loss: 8135.9272\n",
            "Epoch [820], val_loss: 8135.6836\n",
            "Epoch [840], val_loss: 8135.4727\n",
            "Epoch [860], val_loss: 8135.3203\n",
            "Epoch [880], val_loss: 8135.0796\n",
            "Epoch [900], val_loss: 8135.2168\n",
            "Epoch [920], val_loss: 8134.6992\n",
            "Epoch [940], val_loss: 8134.7241\n",
            "Epoch [960], val_loss: 8134.4868\n",
            "Epoch [980], val_loss: 8134.2422\n",
            "Epoch [1000], val_loss: 8134.0454\n",
            "Epoch [1020], val_loss: 8133.8032\n",
            "Epoch [1040], val_loss: 8133.6187\n",
            "Epoch [1060], val_loss: 8133.4087\n",
            "Epoch [1080], val_loss: 8133.2954\n",
            "Epoch [1100], val_loss: 8132.9922\n",
            "Epoch [1120], val_loss: 8132.8281\n",
            "Epoch [1140], val_loss: 8132.6382\n",
            "Epoch [1160], val_loss: 8132.4453\n",
            "Epoch [1180], val_loss: 8132.4146\n",
            "Epoch [1200], val_loss: 8132.1187\n",
            "Epoch [1220], val_loss: 8131.9473\n",
            "Epoch [1240], val_loss: 8131.7192\n",
            "Epoch [1260], val_loss: 8131.5273\n",
            "Epoch [1280], val_loss: 8131.2969\n",
            "Epoch [1300], val_loss: 8131.1001\n",
            "Epoch [1320], val_loss: 8130.8984\n",
            "Epoch [1340], val_loss: 8130.7456\n",
            "Epoch [1360], val_loss: 8130.5288\n",
            "Epoch [1380], val_loss: 8130.2754\n",
            "Epoch [1400], val_loss: 8130.1694\n",
            "Epoch [1420], val_loss: 8129.9702\n",
            "Epoch [1440], val_loss: 8129.8345\n",
            "Epoch [1460], val_loss: 8129.6367\n",
            "Epoch [1480], val_loss: 8129.5024\n",
            "Epoch [1500], val_loss: 8129.2554\n",
            "Epoch [1520], val_loss: 8129.0845\n",
            "Epoch [1540], val_loss: 8128.9336\n",
            "Epoch [1560], val_loss: 8128.7168\n",
            "Epoch [1580], val_loss: 8128.5493\n",
            "Epoch [1600], val_loss: 8128.3706\n",
            "Epoch [1620], val_loss: 8128.1289\n",
            "Epoch [1640], val_loss: 8127.9487\n",
            "Epoch [1660], val_loss: 8127.7754\n",
            "Epoch [1680], val_loss: 8127.5454\n",
            "Epoch [1700], val_loss: 8127.5024\n",
            "Epoch [1720], val_loss: 8127.1851\n",
            "Epoch [1740], val_loss: 8127.0337\n",
            "Epoch [1760], val_loss: 8126.8574\n",
            "Epoch [1780], val_loss: 8126.8359\n",
            "Epoch [1800], val_loss: 8126.4985\n",
            "Epoch [1820], val_loss: 8126.3052\n",
            "Epoch [1840], val_loss: 8126.0200\n",
            "Epoch [1860], val_loss: 8125.9570\n",
            "Epoch [1880], val_loss: 8125.7847\n",
            "Epoch [1900], val_loss: 8125.5444\n",
            "Epoch [1920], val_loss: 8125.3384\n",
            "Epoch [1940], val_loss: 8125.1250\n",
            "Epoch [1960], val_loss: 8124.9897\n",
            "Epoch [1980], val_loss: 8124.8462\n",
            "Epoch [2000], val_loss: 8124.6543\n",
            "Epoch [2020], val_loss: 8124.3359\n",
            "Epoch [2040], val_loss: 8124.2593\n",
            "Epoch [2060], val_loss: 8124.1079\n",
            "Epoch [2080], val_loss: 8123.7954\n",
            "Epoch [2100], val_loss: 8123.7476\n",
            "Epoch [2120], val_loss: 8123.4233\n",
            "Epoch [2140], val_loss: 8123.2866\n",
            "Epoch [2160], val_loss: 8122.9888\n",
            "Epoch [2180], val_loss: 8122.9077\n",
            "Epoch [2200], val_loss: 8122.7109\n",
            "Epoch [2220], val_loss: 8122.4604\n",
            "Epoch [2240], val_loss: 8122.2124\n",
            "Epoch [2260], val_loss: 8122.1289\n",
            "Epoch [2280], val_loss: 8121.8906\n",
            "Epoch [2300], val_loss: 8121.7246\n",
            "Epoch [2320], val_loss: 8121.5859\n",
            "Epoch [2340], val_loss: 8121.3926\n",
            "Epoch [2360], val_loss: 8121.1353\n",
            "Epoch [2380], val_loss: 8121.0547\n",
            "Epoch [2400], val_loss: 8120.8638\n",
            "Epoch [2420], val_loss: 8120.6929\n",
            "Epoch [2440], val_loss: 8120.5210\n",
            "Epoch [2460], val_loss: 8120.3164\n",
            "Epoch [2480], val_loss: 8120.0796\n",
            "Epoch [2500], val_loss: 8119.8989\n",
            "Epoch [2520], val_loss: 8119.7729\n",
            "Epoch [2540], val_loss: 8119.5610\n",
            "Epoch [2560], val_loss: 8119.4648\n",
            "Epoch [2580], val_loss: 8119.3579\n",
            "Epoch [2600], val_loss: 8118.9883\n",
            "Epoch [2620], val_loss: 8118.6948\n",
            "Epoch [2640], val_loss: 8118.6172\n",
            "Epoch [2660], val_loss: 8118.3921\n",
            "Epoch [2680], val_loss: 8118.2153\n",
            "Epoch [2700], val_loss: 8118.0337\n",
            "Epoch [2720], val_loss: 8117.8696\n",
            "Epoch [2740], val_loss: 8117.7144\n",
            "Epoch [2760], val_loss: 8117.4297\n",
            "Epoch [2780], val_loss: 8117.4004\n",
            "Epoch [2800], val_loss: 8117.3208\n",
            "Epoch [2820], val_loss: 8116.9258\n",
            "Epoch [2840], val_loss: 8116.7266\n",
            "Epoch [2860], val_loss: 8116.5415\n",
            "Epoch [2880], val_loss: 8116.4258\n",
            "Epoch [2900], val_loss: 8116.2954\n",
            "Epoch [2920], val_loss: 8116.0078\n",
            "Epoch [2940], val_loss: 8115.8711\n",
            "Epoch [2960], val_loss: 8115.7227\n",
            "Epoch [2980], val_loss: 8115.6191\n",
            "Epoch [3000], val_loss: 8115.3335\n",
            "Epoch [3020], val_loss: 8115.2446\n",
            "Epoch [3040], val_loss: 8115.0430\n",
            "Epoch [3060], val_loss: 8114.8579\n",
            "Epoch [3080], val_loss: 8114.7031\n",
            "Epoch [3100], val_loss: 8114.5664\n",
            "Epoch [3120], val_loss: 8114.4062\n",
            "Epoch [3140], val_loss: 8114.1675\n",
            "Epoch [3160], val_loss: 8114.0679\n",
            "Epoch [3180], val_loss: 8113.7974\n",
            "Epoch [3200], val_loss: 8113.6479\n",
            "Epoch [3220], val_loss: 8113.4961\n",
            "Epoch [3240], val_loss: 8113.3789\n",
            "Epoch [3260], val_loss: 8113.0620\n",
            "Epoch [3280], val_loss: 8112.9277\n",
            "Epoch [3300], val_loss: 8112.6831\n",
            "Epoch [3320], val_loss: 8112.4146\n",
            "Epoch [3340], val_loss: 8112.4297\n",
            "Epoch [3360], val_loss: 8112.2749\n",
            "Epoch [3380], val_loss: 8112.1099\n",
            "Epoch [3400], val_loss: 8111.8931\n",
            "Epoch [3420], val_loss: 8111.6655\n",
            "Epoch [3440], val_loss: 8111.6382\n",
            "Epoch [3460], val_loss: 8111.4810\n",
            "Epoch [3480], val_loss: 8111.1641\n",
            "Epoch [3500], val_loss: 8111.0977\n",
            "Epoch [3520], val_loss: 8110.9199\n",
            "Epoch [3540], val_loss: 8110.7891\n",
            "Epoch [3560], val_loss: 8110.6343\n",
            "Epoch [3580], val_loss: 8110.4688\n",
            "Epoch [3600], val_loss: 8110.1772\n",
            "Epoch [3620], val_loss: 8110.2319\n",
            "Epoch [3640], val_loss: 8109.8535\n",
            "Epoch [3660], val_loss: 8109.6797\n",
            "Epoch [3680], val_loss: 8109.4819\n",
            "Epoch [3700], val_loss: 8109.1953\n",
            "Epoch [3720], val_loss: 8109.1562\n",
            "Epoch [3740], val_loss: 8108.9116\n",
            "Epoch [3760], val_loss: 8108.7310\n",
            "Epoch [3780], val_loss: 8108.4565\n",
            "Epoch [3800], val_loss: 8108.1782\n",
            "Epoch [3820], val_loss: 8108.2339\n",
            "Epoch [3840], val_loss: 8107.8345\n",
            "Epoch [3860], val_loss: 8107.6797\n",
            "Epoch [3880], val_loss: 8107.5845\n",
            "Epoch [3900], val_loss: 8107.4028\n",
            "Epoch [3920], val_loss: 8107.1743\n",
            "Epoch [3940], val_loss: 8107.1328\n",
            "Epoch [3960], val_loss: 8106.9375\n",
            "Epoch [3980], val_loss: 8106.6997\n",
            "Epoch [4000], val_loss: 8106.5938\n",
            "Epoch [4020], val_loss: 8106.3022\n",
            "Epoch [4040], val_loss: 8106.1147\n",
            "Epoch [4060], val_loss: 8105.9590\n",
            "Epoch [4080], val_loss: 8105.7095\n",
            "Epoch [4100], val_loss: 8105.7104\n",
            "Epoch [4120], val_loss: 8105.5254\n",
            "Epoch [4140], val_loss: 8105.3008\n",
            "Epoch [4160], val_loss: 8105.2168\n",
            "Epoch [4180], val_loss: 8105.0044\n",
            "Epoch [4200], val_loss: 8104.7837\n",
            "Epoch [4220], val_loss: 8104.6626\n",
            "Epoch [4240], val_loss: 8104.5181\n",
            "Epoch [4260], val_loss: 8104.4316\n",
            "Epoch [4280], val_loss: 8104.1938\n",
            "Epoch [4300], val_loss: 8104.0220\n",
            "Epoch [4320], val_loss: 8103.7407\n",
            "Epoch [4340], val_loss: 8103.6499\n",
            "Epoch [4360], val_loss: 8103.4199\n",
            "Epoch [4380], val_loss: 8103.2993\n",
            "Epoch [4400], val_loss: 8103.1108\n",
            "Epoch [4420], val_loss: 8103.0591\n",
            "Epoch [4440], val_loss: 8102.9312\n",
            "Epoch [4460], val_loss: 8103.0259\n",
            "Epoch [4480], val_loss: 8102.3960\n",
            "Epoch [4500], val_loss: 8102.3960\n",
            "Epoch [4520], val_loss: 8102.2515\n",
            "Epoch [4540], val_loss: 8102.0742\n",
            "Epoch [4560], val_loss: 8102.0430\n",
            "Epoch [4580], val_loss: 8101.9810\n",
            "Epoch [4600], val_loss: 8101.5835\n",
            "Epoch [4620], val_loss: 8101.4219\n",
            "Epoch [4640], val_loss: 8101.3477\n",
            "Epoch [4660], val_loss: 8101.0981\n",
            "Epoch [4680], val_loss: 8101.0742\n",
            "Epoch [4700], val_loss: 8100.8184\n",
            "Epoch [4720], val_loss: 8100.7656\n",
            "Epoch [4740], val_loss: 8100.2759\n",
            "Epoch [4760], val_loss: 8100.4497\n",
            "Epoch [4780], val_loss: 8100.3687\n",
            "Epoch [4800], val_loss: 8099.9927\n",
            "Epoch [4820], val_loss: 8099.6929\n",
            "Epoch [4840], val_loss: 8099.8267\n",
            "Epoch [4860], val_loss: 8099.5293\n",
            "Epoch [4880], val_loss: 8099.2891\n",
            "Epoch [4900], val_loss: 8099.1694\n",
            "Epoch [4920], val_loss: 8098.9233\n",
            "Epoch [4940], val_loss: 8098.7944\n",
            "Epoch [4960], val_loss: 8098.5366\n",
            "Epoch [4980], val_loss: 8098.4336\n",
            "Epoch [5000], val_loss: 8098.3306\n",
            "Epoch [5020], val_loss: 8098.3950\n",
            "Epoch [5040], val_loss: 8097.8613\n",
            "Epoch [5060], val_loss: 8097.6060\n",
            "Epoch [5080], val_loss: 8097.5469\n",
            "Epoch [5100], val_loss: 8097.3086\n",
            "Epoch [5120], val_loss: 8097.3179\n",
            "Epoch [5140], val_loss: 8097.1860\n",
            "Epoch [5160], val_loss: 8097.0718\n",
            "Epoch [5180], val_loss: 8097.2788\n",
            "Epoch [5200], val_loss: 8096.5918\n",
            "Epoch [5220], val_loss: 8096.6118\n",
            "Epoch [5240], val_loss: 8096.4663\n",
            "Epoch [5260], val_loss: 8096.2954\n",
            "Epoch [5280], val_loss: 8096.2676\n",
            "Epoch [5300], val_loss: 8096.3398\n",
            "Epoch [5320], val_loss: 8095.8711\n",
            "Epoch [5340], val_loss: 8095.5337\n",
            "Epoch [5360], val_loss: 8095.5078\n",
            "Epoch [5380], val_loss: 8095.0962\n",
            "Epoch [5400], val_loss: 8095.1758\n",
            "Epoch [5420], val_loss: 8094.8320\n",
            "Epoch [5440], val_loss: 8094.7739\n",
            "Epoch [5460], val_loss: 8094.4048\n",
            "Epoch [5480], val_loss: 8094.7051\n",
            "Epoch [5500], val_loss: 8094.2871\n",
            "Epoch [5520], val_loss: 8094.1602\n",
            "Epoch [5540], val_loss: 8094.1777\n",
            "Epoch [5560], val_loss: 8094.0562\n",
            "Epoch [5580], val_loss: 8093.5547\n",
            "Epoch [5600], val_loss: 8093.5220\n",
            "Epoch [5620], val_loss: 8093.2583\n",
            "Epoch [5640], val_loss: 8092.9165\n",
            "Epoch [5660], val_loss: 8093.2524\n",
            "Epoch [5680], val_loss: 8092.8657\n",
            "Epoch [5700], val_loss: 8092.6167\n",
            "Epoch [5720], val_loss: 8092.3647\n",
            "Epoch [5740], val_loss: 8092.3296\n",
            "Epoch [5760], val_loss: 8092.1304\n",
            "Epoch [5780], val_loss: 8091.9038\n",
            "Epoch [5800], val_loss: 8091.8101\n",
            "Epoch [5820], val_loss: 8091.5728\n",
            "Epoch [5840], val_loss: 8091.7290\n",
            "Epoch [5860], val_loss: 8091.5469\n",
            "Epoch [5880], val_loss: 8091.2085\n",
            "Epoch [5900], val_loss: 8091.2734\n",
            "Epoch [5920], val_loss: 8090.8359\n",
            "Epoch [5940], val_loss: 8090.6431\n",
            "Epoch [5960], val_loss: 8090.6328\n",
            "Epoch [5980], val_loss: 8090.2729\n",
            "Epoch [6000], val_loss: 8090.0688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcF2PNjLV5rO"
      },
      "source": [
        "**Q: What is the final validation loss of your model?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ3bf97IV5rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42a175d-03f7-4f23-ae83-2d71bcd3be79"
      },
      "source": [
        "val_loss = evaluate(model, val_ds)\r\n",
        "print(val_loss)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'val_loss': 7817.03759765625}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1HfQhacV5rO"
      },
      "source": [
        "Let's log the final validation loss to Jovian and commit the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rluDi-agV5rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c9ecc2-2c9c-4a0d-d19b-54121f12fd2d"
      },
      "source": [
        "jovian.log_metrics(val_loss=val_loss)"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Metrics logged.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2SItZNJV5rP"
      },
      "source": [
        "# jovian.commit(project=project_name, environment=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RDIO_wV5rP"
      },
      "source": [
        "Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7BBN95mV5rP"
      },
      "source": [
        "## Step 5: Make predictions using the trained model\n",
        "\n",
        "**Q: Complete the following function definition to make predictions on a single input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPgsq8VSV5rP"
      },
      "source": [
        "def predict_single(input, target, model):\n",
        "    inputs = input.unsqueeze(0)\n",
        "    predictions = model(input)                # fill this\n",
        "    prediction = predictions[0].detach()\n",
        "    print(\"Input:\", input)\n",
        "    print(\"Target:\", target)\n",
        "    print(\"Prediction:\", prediction)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpgxP1MvV5rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1239908-e617-45a5-cf64-e52b4bf8e83a"
      },
      "source": [
        "input, target = val_ds[0]\n",
        "predict_single(input, target, model)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: tensor([22.0000,  1.0000, 32.4764,  0.0000,  0.0000])\n",
            "Target: tensor([1948.0496])\n",
            "Prediction: tensor(3811.3469)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaE2gVFNV5rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d3cf6c-a5a5-41c3-f8cf-690a55d8011f"
      },
      "source": [
        "input, target = val_ds[10]\n",
        "predict_single(input, target, model)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: tensor([29.0000,  0.0000, 37.7036,  0.0000,  0.0000])\n",
            "Target: tensor([4614.0068])\n",
            "Prediction: tensor(5511.8315)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofqBzt3V5rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf29caba-de9f-42a3-d7db-5774783ea06f"
      },
      "source": [
        "input, target = val_ds[23]\n",
        "predict_single(input, target, model)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: tensor([18.0000,  0.0000, 46.3188,  0.0000,  0.0000])\n",
            "Target: tensor([16535.6543])\n",
            "Prediction: tensor(1558.3708)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhRidByzV5rQ"
      },
      "source": [
        "Are you happy with your model's predictions? Try to improve them further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwpgoHDTV5rQ"
      },
      "source": [
        "## (Optional) Step 6: Try another dataset & blog about it\n",
        "\n",
        "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to replicate this notebook for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patterns in machine learning from problem-specific details.You can use one of these starer notebooks (just change the dataset):\n",
        "\n",
        "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n",
        "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n",
        "\n",
        "Here are some sources to find good datasets:\n",
        "\n",
        "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n",
        "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n",
        "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n",
        "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n",
        "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
        "- https://pytorch.org/docs/stable/torchvision/datasets.html\n",
        "\n",
        "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n",
        "\n",
        "- Interesting title & subtitle\n",
        "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n",
        "- Downloading & exploring the data\n",
        "- Preparing the data for training\n",
        "- Creating a model using PyTorch\n",
        "- Training the model to fit the data\n",
        "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n",
        "- Making predictions using the model\n",
        "\n",
        "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n",
        "\n",
        "Don't forget to share your work on the forum: https://jovian.ai/forum/t/linear-regression-and-logistic-regression-notebooks-and-blog-posts/14039"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EhUf38WV5rQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "8f3275e7-2bee-44e2-f862-15cc4dba31e7"
      },
      "source": [
        "jovian.commit(project=project_name, environment=None)\n",
        "# jovian.commit(project=project_name, environment=None) # try again, kaggle fails sometimes"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
            "[jovian] Committed successfully! https://jovian.ai/ayushxx7/02-insurance-linear-regression\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://jovian.ai/ayushxx7/02-insurance-linear-regression'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGC8eKdV5rQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}